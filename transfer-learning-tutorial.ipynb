{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Step by Step Guide to Transfer Learning with Pytorch: Achieving High Accuracy on the CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Python Programming at an intermediate level, especially good familiarity with Numpy and Pandas\n",
    "* Some software development experience would be helpful but not strictly required. If you are a student with reasonable programming background, you will do just fine.\n",
    "* Should have Anaconda for Python 3.x with Jupyter Notebook installed\n",
    "* Preferably a commodity Nvidia GPU (GTX 1050 Ti, or a flavor of 1060,1070 or 1080 would do just fine. Of course if you have anything better it would only improve things) either locally or rented in Cloud. \n",
    "#### Warning: If you work with CPU only, it may take significantly more time (in order of days) to achieve the same results\n",
    "* Previous knowledge of Pytorch or transfer learning is not required, but the reader should be familiar with basic concepts of Deep Learning and fundamentals of a Neural Network, as well as some basic terminology of Convolutional Neural Networks. Examples of such as fundamentals include Linear functions, Non-Linear transformations such as Rectified Linear Unit (ReLU) etc. back-propagation, gradients and gradient descent algorithm, loss-functions and how the weights of a Neural Network are updated. In terms of CNN, it would help to know how basic convolutions and pooling operations work.\n",
    "\n",
    "We will cover the basics of Pytorch Tensors and Tensor operations and introduce all concepts related to Transfer Learning and Pytorch as we move along with the tutorial.\n",
    "\n",
    "## Introduction to Pytorch\n",
    "Pytorch is a relatively new Deep Learning framework from Facebook that is quickly gaining popularity in the research as well as the developer community. Its primary merits are flexibility and more control over the run-time behavior (sometimes called dynamic behavior) of a Neural Network. Another key advantage is its use of commonly used Python programming patterns and practices, unlike for example, Tensorflow which defines its own special kind of syntax and programming style on top of Python that makes it somewhat harder to learn for newcomers.\n",
    "\n",
    "Pytorch 1.0 version has just been released as beta, at the time of this writing which is a major upgrade in terms of model deployment in the real-world. It introduces a Just-In-Time (JIT) graph compiler through a mechanism called Torch Script that makes it more efficient to deploy a model for prediction. \n",
    "However, in this tutorial we shall be using version 0.4.1 which is the last one before this major upgrade.\n",
    "\n",
    "## Introduction to Transfer Learning\n",
    "When we take a model created and trained elsewhere on a similar problem that we are trying to solve, and reuse its architecture and (possibly) its weights in our setting, we are applying Transfer Learning. It means that somebody trained a Neural Network model, on most likely a very large dataset, and put that pre-trained model in a model repository. We take that model and modify it a little bit to adapt it to our use case, thus transferring the learning achieved by that model previously to our application, without having to retrain it from scratch. This not only saves time but also transfers the ”knowledge” of the model to our case, which usually results in achieving very high accuracy. \n",
    "\n",
    "Essentially, we are building on other people’s work who make it available for the greater good. It’s a great step towards democratization of deep learning and Artificial Intelligence in general. Transfer learning is a highly effective technique used throughout the world by Deep Learning practitioners today. \n",
    "\n",
    "Transfer Learning is most effective when the use case is well-understood and the data is sort of ”fixed” so to speak, for example, image classification and object detection which are based on just pixels, or Natural Language Processing (NLP) text corpuses which are words out of a large vocabulary. \n",
    "\n",
    "It may not be that effective for structured or tabular data used in business settings e.g. data collected from databases and files because one company’s data may be quite different in structure and semantics from others. However, even that is changing now with recent trend in the use of categorical embeddings just like word embeddings used in NLP. Such embeddings allow us to transfer the learning achieved through data of one organization for a specific domain (e.g. predicting retail sales) to similar problems of others in the same domain.\n",
    "\n",
    "## Our Problem Deifinition\n",
    "In this tutorial, we provide a step-by-step guide to applying Transfer Learning in Pytorch on an image classification problem. The problem is to automatically classify objects present in images into categories e.g. bird, plane, dog, cat etc.\n",
    "\n",
    "## Image Classification Use Cases\n",
    "Image Classification is the basis and a core building block of several complex applications such as object detection, image captioning, face recognition and image segmentation to name a few. Features extracted from images during classification can be effectively used in several use cases and applications related to Computer Vision. \n",
    "\n",
    "## The Dataset\n",
    "We will be using Cifar10 dataset. It is a dataset consisting of 60000 images categorized into 10 classes. Each image is of size 28x28. The images being small and somewhat blurry (low resolution) makes it one of the more difficult data-sets for classification. Some of the available benchmarks for this dataset are given at: <https://benchmarks.ai/cifar-10> \n",
    "\n",
    "There was a Kaggle competition on Cifar10 in 2014, whose results are also available at <https://www.kaggle.com/c/cifar-10>\n",
    "\n",
    "\n",
    "## Objectives\n",
    "At the end of this tutorial, the readers should be able to:\n",
    "* Create an API (set of classes and utility functions) with Pytorch to preprocess and prepare any image dataset for training, evaluation and prediction\n",
    "* Construct and use an API to effectively apply Transfer Learning in Pytorch on an image dataset for classification\n",
    "* Acquire some tips and tricks to achieve very high accuracy on Cifar10 using three different, freely available pre-trained models by combining them effectively to achieve higher accuracy than the individual models. \n",
    "* Know how to create their own classes for Deep Learning tasks with Pytorch and use them as components in other applications\n",
    "\n",
    "## State of the art results\n",
    "While preparing this tutorial, my accuracy (94.7%) ended up on third place on both, the benchmark site as well as Kaggle scoring (of course through late submission). This was achieved in less than 2 hours of training altogether (all three models combined) on a commodity Nvidia GTX 1070 GPU. \n",
    "\n",
    "I will show you some simple tips and tricks to increase accuracy of your models with transfer learning and also how to ensemble different models together to achieve even higher accuracy in most applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Outline of the Tutorial\n",
    "This tutorial proceeds through the following steps:\n",
    "    1. Create Pytorch Dataset for Cifar10\n",
    "    2. Pre-process the Dataset and prepare it for training\n",
    "    3. Create a Base Class for building a basic Neural Network\n",
    "    4. Create a Fully Connected Class derived from the Base Class\n",
    "    5. Create a Transfer Learning Class derived from the Base Class \n",
    "    6. Train two different pretrained, transferred models on Cifar10 dataset\n",
    "    7. Evaluate and predict on test set with individual models and Ensemble\n",
    "    8. Predict on Kaggle's given much larger Test set\n",
    "\n",
    "### Code Comments\n",
    "Code blocks and snippets have been explained using multiline comments on top of each block where I thought was necessary to explain something. Please pay attention to the code comments in \"red\". Previously explained pieces of code have been replaced by Ellipses (...) in subsequent code blocks for brevity.\n",
    "\n",
    "### Link to code\n",
    "The complete code is available on Git-Hub at: <>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch ## for pytorch\n",
    "import torchvision ## for transfer learnhing models and many other vision related classes\n",
    "from torch import nn ## Core Neural Network Model classes in Pytorch\n",
    "from torch import optim ## Contains several Pytorch optimizer classes\n",
    "import torch.nn.functional as F ## Contains several utilily functions provided by Pytorch\n",
    "\n",
    "from torchvision import datasets, transforms, models ## Many Computer Vision related classes\n",
    "                                                     ## for datasets and transformations etc.\n",
    "from torch.utils.data import * ## Contains several utilily functions for dataset manipulation\n",
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The following impports contain classes and functions that we develop throughout this tutorial. They have\n",
    "## been explained throughout this tutorial.\n",
    "\n",
    "from mylib.utils import *\n",
    "from mylib.model import *\n",
    "from mylib.cv_model import *\n",
    "from mylib.fc import *\n",
    "from mylib.chkpoint import *\n",
    "from mylib.cv_data import *\n",
    "\n",
    "\n",
    "## The following two lines are for reloading any imported files if they are modified while \n",
    "## our Jupyter Notebook is running\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Create a Pytorch Dataset for CIFAR 10\n",
    "* We download the train and test datasets using CIFAR10 constructor available in datasets module in torchvision. \n",
    "* We pass train=True flag first to indicate that we want the training set. Then we pass it as False to download the test set\n",
    "* We pass download=True since it is the first time we are constructing this dataset. Therefore, it will download first from a prespecified URL within the CIFAR10 class. \n",
    "* After running this cell first time and successfully downloading the datasets, you should change it to False to avoid downaloading every time.\n",
    "* The result of the following operations would be two dataset objects representing the CIFAR10 training set and test sets respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = datasets.CIFAR10('Cifar10', train=True,\n",
    "                              download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10('Cifar10', train=False,\n",
    "                             download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us two dataset objects which are of torchvision.datasets.cifar.CIFAR10 type. This is a sub-class of Pytorch' Dataset class which is the main class to generically represent any dataset. This particular class represents CIFAR10 data stored in its internal data structure. Later these objects shall be passed to a Pytorch Dataloader objects (explained later) for processing the images.\n",
    "\n",
    "We can veify the lengths (number of images) of both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, we have 50000 and 10000 images in training and test sets respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick refresher of Tensors\n",
    "Tensors are just a way of representing n-dimensional data objects of a single type (integers or float etc) in a generic way. for example:\n",
    "* A single value (integer or float) is a 0-dimensional tensor\n",
    "* An array with N elements is a one-dimentional tensor\n",
    "* A matrix with M rows and N columns is a 2-dimensional tensor (MxN)\n",
    "* An MxN image with three RGB (Red, Green Blue) color channels represented by three matrices is a three dimenional tensor (3 x M x N)\n",
    "\n",
    "\n",
    "The image tensors are contained in the field train_data within the dataset object. Let's look at the shape of one of the tensors representing an image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.train_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that our images are of 32 x 32 in size with 3 color channels.\n",
    "\n",
    "Let's look at some of the images using matplotlib.plyplot module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f71447fa7f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_dataset.train_data[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a ship. As you can observe, the images are rather blurry and quite low resolution (32 x 32)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Pre-process the Dataset and prepare it for training\n",
    "* Understand the concept of Data loader and the Pytorch Data loader API\n",
    "* Split the images into train, validation and test sets\n",
    "* Create Pytorch Dataloaders to feed images while training, validation and prediction\n",
    "* Use Pytorch API to define Transforms for preprocessing the Dataset for more effective training\n",
    "* Use Pytorch API to convert all images to Pytorch Tensors\n",
    "* Normalize the dataset using mean and standard deviation of images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders\n",
    "Pytorch Dataloaders are objects that act as Python generators. They supply data in chunks or batches while training and validation. We can instantiate Dataloader objects and pass our datasets to them. Dataloaders store the dataset objects internally. \n",
    "\n",
    "When the application asks for the next batch of data, a dataloader uses its stored dataset as a Python iterator to get the next element (row or image in our case) of data. Then it aggregates a batch worth of data and returns it to the application.\n",
    "\n",
    "Following is an example of calling the Dataloader constructor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=50,sampler=SubsetRandomSampler(indices),\n",
    "                                           num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [4] Here we are creating a Dataloader object for our training dataset with a batch size of 50. \n",
    "* The sampler parameter specifies the strategy with which we want to sample data while constructing batches.\n",
    "* We have different samplers available in torch.utils.data.sampler. The explanation is straightforward. You can read about them in the Pytorch Documentation at https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler.\n",
    "\n",
    "* The num_workers argument specifies how many processes (or cores) we want to use while loading our data. This provides parallelism while loading large datasets. Default is 0 which means load all data in main process.\n",
    "\n",
    "\n",
    "Dataloader reports its length in number of batches. Since we created this Dataloader with a batch size of 50 and we had 50000 images in our train dataset, we have the length of dataloader = 1000 batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data\n",
    "Now let's write a function to split our datasets into train, validation and test sets and create their corresponding dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes the train and test data sets as arguments. \n",
    "Test Data can be None in which case it splits train data into \n",
    "three sets,train, test and validation.\n",
    "If test_data is not none it just splits train set into \n",
    "train and validation and creates a separate dataloader \n",
    "from test set\n",
    "'''\n",
    "\n",
    "def split_image_data(train_data,\n",
    "                     test_data=None,\n",
    "                     batch_size=20,\n",
    "                     num_workers=0,\n",
    "                     valid_size=0.2,\n",
    "                     sampler=SubsetRandomSampler):\n",
    "    \n",
    "    num_train = len(train_data)\n",
    "    \n",
    "    '''\n",
    "    It creates a list of indices from the train set \n",
    "    using Python range function on its length\n",
    "    '''\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    '''\n",
    "    It splits the indices list according to the given\n",
    "    validation set size (valid_size argument) whose\n",
    "    default is 0.2 (20% of train data set aside for validation)\n",
    "    '''\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    '''\n",
    "    It uses RandomSubsetSampler constructor to shuffle the train \n",
    "    and validation set indices\n",
    "    '''\n",
    "    train_sampler = sampler(train_idx)\n",
    "    valid_sampler = sampler(valid_idx)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    If a separate test set is given, it simply creates a Dataloader \n",
    "    from that set.\n",
    "    \n",
    "    If no test set is given, it further splits the train\n",
    "    indices (which were obtained by splitting the original train_set \n",
    "    into train and validation indices earlier), into a set of train and\n",
    "    test indices. Note that the test indices size is equal to the \n",
    "    validation set.\n",
    "    \n",
    "    This results in a new set of indices and a sampler \n",
    "    for test set from the train set.\n",
    "    '''\n",
    "    \n",
    "    if test_data is not None:\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size,\n",
    "        num_workers=num_workers)\n",
    "    else:\n",
    "        train_idx, test_idx = train_idx[split:],train_idx[:split]\n",
    "        train_sampler = sampler(train_idx)\n",
    "        test_sampler = sampler(test_idx)\n",
    "        \n",
    "        test_loader = torch.utils.data.DataLoader(train_data, \n",
    "                                                  batch_size=batch_size,\n",
    "                                                  sampler=test_sampler,\n",
    "                                                  num_workers=num_workers)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=train_sampler,\n",
    "                                               num_workers=num_workers)\n",
    "    \n",
    "    valid_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size, \n",
    "                                               sampler=valid_sampler,\n",
    "                                               num_workers=num_workers)\n",
    "    \n",
    "    return train_loader,valid_loader,test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call this function to obtain our Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 200, 200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainloader,validloader,testloader = split_image_data(train_dataset,test_dataset,batch_size=50)\n",
    "\n",
    "len(trainloader),len(testloader),len(validloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have a nice split with 800 batches in our train set and 200 each in our validation and test sets respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (CNN)\n",
    "Although, we assume that you have a basic understanding of CNNs, if you want to refresh the core concepts, folowing are some great tutorials:\n",
    "[Convolutional Neural Networks CS231n Stanford](http://cs231n.github.io/convolutional-networks/)\n",
    "\n",
    "[CNN Tutorial: AnalyticsVidhya](https://www.analyticsvidhya.com/blog/2018/12/guide-convolutional-neural-network-cnn/)\n",
    "\n",
    "[A Very Comprehensive Tutorial on ANN and CNN by Kaggle](https://www.kaggle.com/shivamb/a-very-comprehensive-tutorial-nn-cnn)\n",
    "\n",
    "### Preprocessing and Transforming the Dataset\n",
    "Before we move on to defining our Network and start training, we need to preprocess our datasets. Specifically, we need to perform the following steps:\n",
    "* Resize the images to an appropriate size for our models\n",
    "* Perform some basic and most common data augmentation\n",
    "* Convert the image dat to Pytorch Tensors \n",
    "* Normalize the image data\n",
    "\n",
    "###  Why do we want to Resize Images?\n",
    "Most of our Transfer Learning models require data to be of at least 224x224 size. The reason for this limitation is that these models are designed with a large number of Convolution and pooling layers, finally followed by a fully connected (Linear) layer at the end to generate the classification output. By the time the input image reaches the final layer, it has been reduced drastically in size due to the way convolutions and pooling are defined. If the input image was already too small (like 32x32 CIFAR10 images in our case), it would be too small for the network to produce any significant output. Therefore, these models sort of restrict us to input an image >=224x224.\n",
    "\n",
    "Please note that we wouldn't have needed resizing if our images were already > 224x224, like in case of ImageNet, or if we were to use our own CNN architecture which did not reduce the image size too much while passing it through layers. Resizing smaller images to larger ones (as in our case) creates artifacts that we don't (ideally) want our model to learn. Since our images are really small in case of CIFAR10 and the transfer learning models we are using have this requirement, we are obliged to resize.\n",
    "\n",
    "In case of datasets with larger images, our GPU or CPU memory constraints may become a factor. Therefore, we combine downsizing with increased batch sizes (till we hit the batch size limit) to optimize the model performance and balance the effects of down-sizing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "Data Augmentation is a common technique with Deep learning where we modify images on the fly while training to make the neural network see additional images flipped or rotated at different axes and angles. This usually results in better training performance since the Network sees multiple views of the same image and has a better chance of identifying its class when minimizing the loss function. \n",
    "\n",
    "Note that the augmented images are not added to the dataset, they are just created while generating batches, so the actual images seen during training would increase but you won't see the number of images in the datasets increasing. The length and other functions that count the number of images would still give the same answer.\n",
    "We use two commonly used augmentations below: \n",
    "* RandomHorizontaFlip that flips some of the images around the vertical axis with a probability p that defaults to 0.5 meaning that  50% of the images shall be flipped\n",
    "* RadomRotation at a specific degree (10 in our case below) that rotates some of them randomly at an angle of 10 degree again with a probability of p which defaults to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "train_transform = transforms.Compose([transforms.Resize(224),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomRotation(10),\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.CIFAR10('Cifar10',download=False,transform=train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "In data normalization we statistically normalize the pixel values in our images. This mostly results in better training performance and faster convergence. A common way to perform normalization is to subract the mean of pixel values of the whole dataset from each pixel, and then divide by the standard deviation of the pixels of whole dataset.\n",
    "\n",
    "* The most common way in Transfer learning is to use the mean and std values of the dataset the original Transfer Learning model was trained on. However, it may be a good strategy for cases where we don't want to retrain any part of the original model. \n",
    "* If our dataset is large and we want to retrain whole or part of the original model, then we would be **better off normalizing with the mean and standard deviation of the dataset in question (CIFAR10 in our case)**. However, in most transfer learning tutorials you'll find, the mean and std values for ImageNet are used.\n",
    "\n",
    "Below, I give you two functions to calculate the mean and std of a dataset:\n",
    "\n",
    "First one, \"calculate_img_stats_avg\" is based on Dataloader and calculates means and stds of each batch of data as it is retrieved from the dataset object, and finally takes the average of the accumulated means and std values. Although, this gives us an approximation of the actual values, it is reasonable to use for large datasets that won't fit into memory at the same time. This code has been adapted from [Pytorch forum](https://discuss.pytorch.org/t/about-normalization-using-pre-trained-vgg16-networks/23560/6?u=ptrblck)\n",
    "\n",
    "The second function, \"calculate_img_stats_full\" calculates the actual mean and std of the whole dataset by working on it at once. This would give more accurate values, although, would most likely run out of memory for large datasets. For CIFAR10, this function requires 28GB of RAM. My machine has 32GB but it falls short and I am unable to run this function. This code has been adapted from the book \"Deep Learning with Pytorch\" by Eli Stevens and Luca Antiga, Manning Publications. \n",
    "\n",
    "You can try to run the second function on your specific dataset and if you run into memory issues, then fall-back to the first one for a good approximation. In case of CIFAR10 however, many people have calculated the mean and std of the dataset and the values are well known, like ImageNet. We are using those values in the code that follows. I did not try with the approximate values given by the first function but you are welcome to try with those.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "dataset = datasets.CIFAR10('Cifar10',download=False,transform=transform)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=50,num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We first create a dataset from full data and then a dataloader to feed the data in batches of size 50 to \n",
    "our loop.\n",
    "* Note that for Dataloader to work, the images have to be converted to a Tensor, so that is the only transform we are using.\n",
    "* The function below is straight forward implementation that calculates mean and std of each batch and add them to their cumulative sums, dividing in the end by the total number of batches to get the averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_img_stats_avg(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    nb_samples = 0.\n",
    "    for imgs,_ in loader:\n",
    "        batch_samples = imgs.size(0)\n",
    "        imgs = imgs.view(batch_samples, imgs.size(1), -1)\n",
    "        mean += imgs.mean(2).sum(0)\n",
    "        std += imgs.std(2).sum(0)\n",
    "        nb_samples += batch_samples\n",
    "\n",
    "    mean /= nb_samples\n",
    "    std /= nb_samples\n",
    "    return mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4914, 0.4822, 0.4465]), tensor([0.2023, 0.1994, 0.2010]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_img_stats_avg(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_img_stats_full(dataset):\n",
    "    imgs_ = torch.stack([img for img,_ in dataset],dim=3)\n",
    "    imgs_ = imgs_.view(3,-1)\n",
    "    imgs_mean = imgs_.mean(dim=1)\n",
    "    imgs_std = imgs_.std(dim=1)\n",
    "    return imgs_mean,imgs_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_img_stats_full(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* torch.stack function above stacks the data along the given dimension (3 in our case). The view operation views the Tensor as a 3 x (product of all other dimensions) which basically flattens while keeping the first dimension to be 3\n",
    "\n",
    "* The best way to visualize what is going on in an obscure kind of function as this one is to copy isolate the statements and feed them some dummy tensors to see what's going on. I leave it for you as an exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values below have been taken from the same book (referred above from which the code has been taken):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_mean = [0.4915, 0.4823, 0.4468]\n",
    "cifar10_std  = [0.2470, 0.2435, 0.2616]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our Datasets again from scratch with all the transformations, augmentations and normalization applied, splitting them into train and test and obtaining the final Dataloaders. **Note that we also define our batch size = 50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 200, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "'''\n",
    "ToTensor() converts a numpy array (all our images are constructed as \n",
    "numpy arrays by the Dataset class when read from disk).\n",
    "\n",
    "Normalize() is another transform that normalizes according to the passed \n",
    "values of Means and STD of each channel as separate lists or tuples.\n",
    "'''\n",
    "train_transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomRotation(10),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(cifar10_mean, cifar10_std)\n",
    "                                     ])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(cifar10_mean, cifar10_std)\n",
    "                                    ])\n",
    "\n",
    "train_data = datasets.CIFAR10('Cifar10', train=True,\n",
    "                              download=False, transform=train_transform)\n",
    "test_data = datasets.CIFAR10('Cifar10', train=False,\n",
    "                             download=False, transform=test_transform)\n",
    "\n",
    "trainloader,validloader,testloader = split_image_data(train_data,test_data,batch_size=batch_size)\n",
    "\n",
    "len(trainloader),len(testloader),len(validloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation is (mostly) applied to Train Set only\n",
    "Note that we usually don't apply data augmentation to test set because we want the test data to remain as close to real data as possible, otherwise there's a chance that we may over-estimate performance. For example, our model may have mis-classified a test image but were correct for its flipped and rotated versions. This would increase the overall accuracy which would be misleading.\n",
    "\n",
    "Having said that, there is a technique called [Test Time Augmentation (TTA)](https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d) where we augment test data and average out the predictions after showing the trained model all the (augmented) variations of an image with the original one while testing. This may result in better accuracy sometimes. We are not going to use it in this tutorial but you can find out more in [this tutorial](https://www.kaggle.com/andrewkh/test-time-augmentation-tta-worth-it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create a Base Class for building a basic Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our Dataloaders all prepared, we are ready to define our Neural Network and train it. In order to define a Neural Network, the best way is to define classes that isolate and abstract out functionality common to all types of Networks like training loops, validation, evaluation, prediction, setting different hyper-parameters etc. \n",
    "\n",
    "We also need to define classes that implement specific type of Networks e.g. specialized for Transfer Learning, or tailor-made for Fully Connected operation etc. Keeping this in mind, we will create three main classes:\n",
    "\n",
    "* A Base Class representing a Neural Network derived from Pytorch's core nn.Module class \n",
    "which is the foundation of any Neural Network in Pytorch\n",
    "* A class derived from our base class that implements functionality specific to Transfer Learning\n",
    "* A class derived from our base class that implements functionality specific to Fully Connected Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's build our base class called Network step by step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " We inherit our class from nn.Module which is the core Pytorch class \n",
    " for creating Neural Networks\n",
    "'''\n",
    "class Network(nn.Module):\n",
    "    def __init__(self,device=None):\n",
    "        '''\n",
    "         We call the parent's constructor as we do in any \n",
    "         Python class derived from a parent class\n",
    "        '''\n",
    "        super().__init__()\n",
    "        '''\n",
    "        We set the device attribute to 'cuda' if it is available otherwise we set it to 'cpu'.\n",
    "        This will help us avoid putting if else checks everywhere in the code regarding \n",
    "        CUDA availability. We can just move the tensors to whatever device is set on our object.\n",
    "        '''\n",
    "        if device is not None:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    '''\n",
    "    We create a dummy forward method. Forward method is the core method in Pytorch that executes\n",
    "    the Network graph and passes the inputs through the network transforming them and getting\n",
    "    the output at the other end. \n",
    "    In Pytorch, we write our own forward method that executes the modules defined in the __init__ \n",
    "    method at run-time. \n",
    "    Since we will be writing the forward methods in derived classes, it is empty in the base class.\n",
    "    '''\n",
    "    def forward(self,x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Note that forward method is called by nn.Module's \"__ call __\" method. So the object of our calss can become a \"callable\" and when it is called, the forward method shall be automatically invoked**. Please refer to any good Python tutorial if you want to know more about callables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Method\n",
    "Next we add the train method. For training any Neural Network, there are a few common tasks that need to be performed in each iteration of the training loop. The following outline of the training loop is the logic of the inner part of the loop that performs actual training in each epoch. This part of the code goes through each batch. It basically defines a **single epoch** (single pass through the whole dataset):\n",
    "* Get the next batch of data\n",
    "* Move the Tensors of the batch to the device (GPU or CPU)\n",
    "* Zero out the gradients of all weights \n",
    "* Call the forward function to send the inputs through the Network\n",
    "* Pass the outputs obtained to the criterion (loss function) to compare them against the labels (targets) and calculate the loss\n",
    "* Calculate the gradients\n",
    "* update all the weights according to the gradients and the learning rate\n",
    "* update the overall loss within this epoch.\n",
    "\n",
    "If you are familiar with the basics of Neural Network, you must have recognized these steps since they are common to all frameworks and Neural Network types. Following code in train_ method performs these steps. Although, the code is pretty self-explanatory, a quick summary of Pytorch specific functions follows the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    ...\n",
    "    \n",
    "    '''\n",
    "    print_every indicates after how many batches we want to print the information about loss\n",
    "    '''\n",
    "    def train_(self,trainloader,criterion,optimizer,print_every):\n",
    "        '''\n",
    "        train method below (self.train()) is a built-in Pytorch method in the base class (nn.Module) \n",
    "        that sets a flag on the model object indicating that training is in progress. \n",
    "        This flag is used by several Pytorch modules that behave differently during training and \n",
    "        validation/testing e.g. Dropout, batch normalization etc.\n",
    "        '''\n",
    "        self.train()\n",
    "        t0 = time.time()\n",
    "        batches = 0\n",
    "        running_loss = 0\n",
    "        \n",
    "        '''\n",
    "        inputs and labels are one batch of images and their corresponding labels \n",
    "        from the trainloader\n",
    "        '''\n",
    "        for inputs, labels in trainloader:\n",
    "            batches += 1\n",
    "            #t1 = time.time()\n",
    "            \n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.forward(inputs)\n",
    "            '''\n",
    "            Criterion is basically the loss function that calculates the \n",
    "            difference between the output of the network and the actual labels.\n",
    "            '''\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            '''\n",
    "            loss.backward() performs backpropagation, calculating the gradients\n",
    "            througout the network following the complete graph of connected Tensors\n",
    "            '''\n",
    "            loss.backward()\n",
    "            \n",
    "            '''\n",
    "            Optimizer.step performs one step of the optimizer algorithm after the loss function \n",
    "            has executed and the new gradients are available.\n",
    "            '''\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            '''\n",
    "            item() method gives a scalar value. It is used for Tensors \n",
    "            that return a single value (loss is a floating point numerical\n",
    "            value in this case)\n",
    "            '''\n",
    "            loss = loss.item()\n",
    "            \n",
    "            #print('training this batch took {:.3f} seconds'.format(time.time() - t1))\n",
    "            '''\n",
    "            We keep a total running loss for this epoch\n",
    "            '''\n",
    "            running_loss += loss\n",
    "            \n",
    "            '''\n",
    "            Print the loss information if the number of batches has reached print_every since\n",
    "            the last print \n",
    "            '''\n",
    "            if batches % print_every == 0:\n",
    "                print(f\"{time.asctime()}..\"\n",
    "                        f\"Time Elapsed = {time.time()-t0:.3f}..\"\n",
    "                        f\"Batch {batches+1}/{len(trainloader)}.. \"\n",
    "                        f\"Average Training loss: {running_loss/(batches):.3f}.. \"\n",
    "                        f\"Batch Training loss: {loss:.3f}.. \"\n",
    "                        )\n",
    "                t0 = time.time()\n",
    "        \n",
    "        '''\n",
    "        At the end we return the average loss of this epoch\n",
    "        '''\n",
    "        return running_loss/len(trainloader) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Functions\n",
    "Note that Pytorch comes with many built-in loss functions for common cases like classification and regression etc. Here we are passing the loss function to train_ as an argument. Some common loss functions used in classification are CrossEntopy loss, Negative Likehood Log Loss (NLLLoss) and Binary-CrossEntropy). We will discuss more about loss function when we discuss the Fully Connected Class later in this tutorial.\n",
    "#### Optimizer Module\n",
    "Optimizer module applies gradient descent or its variant and performs weight updates with gradients and learning rates. Optimizers come in several flavors with different algorithms and are found in torch.optim module. Examples include Stocahstic Gradient Descent (SGD), Adam, AdaDelta etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Method\n",
    "The task of the validate method is to apply the model to the validation set for evaluation. The purpose is to periodically assess how we are doing in terms of training. If you are familiar with Machine Learning concepts, you most likely know about bias (underfitting) and variance (overfitting). If our loss on validation set is **significantly and consistently** higher than the loss on training set, we are overfitting. This basically means our model won't generlize good enough on any other dataset because we are too tightly bound to the training set.\n",
    "\n",
    "* The idea here is to evaluate the model on validation set after every few epochs (a good default is after every epoch), measure the loss and print it out to see if we are overfitting.\n",
    "\n",
    "* The difference between validate method and train is that in validation we don't need to back-propagate, calculate the gradients, apply gradient descend and update the weights. All we need is to pass the validation data set batch by batch through our model and evaluate the loss using the loss function. \n",
    "\n",
    "* As our model gets better after some epochs, we should see our validation loss going down.\n",
    "\n",
    "* One additional thing we also want to do in validation is to calculate the accuracy of our classification. This is simply the percentage of how many times we are correct in our prediction:\n",
    "   100 x (number of correctly pedicted classes/dataset size)\n",
    "* However, it would be better if we also calculate class-wise accuracy i.e. for each individual class we calculate how many of that class we got right verses the total number of images we have of that class.\n",
    "* So we also write a utility function to calculate class-wise accuracies as shown below. This may come handy when we do predictions on our test set or any other set of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def update_classwise_accuracies(preds,labels,class_correct,class_totals):\n",
    "    \n",
    "    correct = np.squeeze(preds.eq(labels.data.view_as(preds)))\n",
    "    \n",
    "    '''\n",
    "    We simply go through the batch (shape[0] is the batch size) and update \n",
    "    the classwise correct and total counts\n",
    "\n",
    "    '''\n",
    "    for i in range(labels.shape[0]):\n",
    "        label = labels.data[i].item()\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_totals[label] += 1\n",
    "        \n",
    "class Network(nn.Module):\n",
    "    ...\n",
    "    \n",
    "    def validate_(self,validloader):\n",
    "        running_loss = 0.\n",
    "        accuracy = 0\n",
    "        \n",
    "        '''\n",
    "        We create two Python Default dictionaries to store classwise correct predictions \n",
    "        and total images per class\n",
    "        '''\n",
    "        class_correct = defaultdict(int)\n",
    "        class_totals = defaultdict(int)\n",
    "        \n",
    "        '''\n",
    "        self.eval() is a Pytorch method to put the model into evaluation mode. It tells Pytorch \n",
    "        we only want to perform forward pass through the network and no backpropagation.\n",
    "        It is opposite to train method we had in our training loop\n",
    "        '''\n",
    "        self.eval()\n",
    "        \n",
    "        '''\n",
    "        Whatever we put in torch.no_grad() block tells Pytorch not to compute gradients. \n",
    "        We want to make sure that gradients are never calculated within the evaluation loop.\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in validloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.forward(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, preds = torch.max(torch.exp(outputs), 1) # you can safely remove the call to \n",
    "                                                            #torch.exp(as described below)\n",
    "                    \n",
    "                update_classwise_accuracies(preds,labels,class_correct,class_totals)\n",
    "        \n",
    "        '''\n",
    "        We calculate the accuracy by the simple formula we discussed earlier.\n",
    "        '''\n",
    "        accuracy = (100*np.sum(list(class_correct.values()))/np.sum(list(class_totals.values())))\n",
    "        \n",
    "        '''\n",
    "        We put the model back to train mode\n",
    "        '''\n",
    "        self.train()\n",
    "        \n",
    "        '''\n",
    "        Running loss is total loss of all the batches. Dividing it by length of the trainloader \n",
    "        (number of batches) gives us average loss for the whole validation set\n",
    "        '''\n",
    "        return (running_loss/len(validloader),accuracy)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **np.squeeze(preds.eq(labels.data.view_as(preds)))** \n",
    "\n",
    " * This seems like a pretty obscure statement so let's break it down:\n",
    "       * The actual labels are contained in our dataloader's data attribute.\n",
    "       * Predictions are the output of our network\n",
    "       * view_as method reorganizes a tensor according to the dimensions of the tensor passed as the argument. In our case this statement will align the labels in the batch with the predictions tensor i.e. batch_size x 10 since there are 10 classes of our network and our final Fully Connected layer would emit these many outputs for each batch. \n",
    "       * The eq method compares each row of a tensor and emits a 1 (True) where the rows are equal and 0 otherwise.\n",
    "       * The final result would be a 50 x 1 Tensor which we flatten by squeezing out extra batch dimension to make it a 50-dimensional vector (1-dimensional tensor) containing either 1s (where predictions are equal to labels  or 0s where they are unequal.\n",
    "   \n",
    "* **_, preds = torch.max(torch.exp(outputs), 1)**\n",
    " * We will use Log of Softmax alog with Negative Log Likelihood Loss (NLLLoss) in our Fully Connected model (more on this later). Therefore, our outputs are expected to be log of probability values (also called Logits) . We don't strictly need to exponentiate the logits here as the max of the logits would still give us the same class index. We are doing it here just to make our predictions look like probabilities which sometimes helps in debugging. You are free to remove torch.exp call in the code if you want. torch.max returns a tuple containing the maximum value and the index of the maximum value within the tensor. Since the index in our case represents the classified category itself, so we only take that ignoring the actual probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Method\n",
    "The purpose of evaluate method is to assess the performance of our model after training has completed on a test dataset. The assumption is that we have labels available for the dataset we want to pass to this method. \n",
    "\n",
    "The code is almost the same as validate. The only difference is that we don't have to calculate loss in this case since we are done with the training.\n",
    "\n",
    "Since this method returns the overall accuracy as well as class-wise accuracies, we need another utility function get_accuracies. We also need class_names to get the actual names of the classes (if available). We will store the class names as a dictionary mapping ids (numbers) to class name strings when we create our Transfer Learning Model (later in this tutorial).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def update_classwise_accuracies(preds,labels,class_correct,class_totals):\n",
    "    correct = np.squeeze(preds.eq(labels.data.view_as(preds)))\n",
    "    for i in range(labels.shape[0]):\n",
    "        label = labels.data[i].item()\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_totals[label] += 1\n",
    "\n",
    "def get_accuracies(class_names,class_correct,class_totals):\n",
    "    \n",
    "    accuracy = (100*np.sum(list(class_correct.values()))/np.sum(list(class_totals.values())))\n",
    "    \n",
    "    '''\n",
    "    We get the class name and the accuracy of this class by dividing the correct \n",
    "    predictions of this calss by the total number of images of this class we have in the test dataset. \n",
    "    We put an extra condition that we have at least one image of a class to avoid dividing by 0\n",
    "    '''\n",
    "    class_accuracies = [(class_names[i],100.0*(class_correct[i]/class_totals[i])) \n",
    "                        for i in class_names.keys() if class_totals[i] > 0]\n",
    "    return accuracy,class_accuracies\n",
    "\n",
    "class Network(nn.Module):\n",
    "    ...\n",
    "    \n",
    "    def evaluate(self,testloader):\n",
    "        self.eval()\n",
    "        self.model.to(self.device)\n",
    "        class_correct = defaultdict(int)\n",
    "        class_totals = defaultdict(int)\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in testloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.forward(inputs)\n",
    "                ps = torch.exp(outputs)\n",
    "                _, preds = torch.max(ps, 1)\n",
    "                update_classwise_accuracies(preds,labels,class_correct,class_totals)\n",
    "                \n",
    "        self.train()    \n",
    "        return get_accuracies(self.class_names,class_correct,class_totals)\n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Method\n",
    "The predict method is used to predict or draw inference from our trained model to determine the class of images for which we do not have labels. This is the method that would be called when the model is deployed in real life.\n",
    "\n",
    "* It is very similar to evaluate except that there are no labels\n",
    "* Another difference is that we are also interested in probabilities as well as predicted classes.\n",
    "* We may also want to know the predicted probabilities of more than one classes e.g. top 3 most likely classes predicted along with their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    ...\n",
    "    \n",
    "    '''\n",
    "    Since we need probabilities and (possibly) multiple ranked classes, \n",
    "    we pass the topk argument that tells our function how many ranked\n",
    "    classes with their probabilities we have to return.\n",
    "\n",
    "    '''\n",
    "     def predict(self,inputs,topk=1):\n",
    "        self.eval()\n",
    "        self.model.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            inputs = inputs.to(self.device)\n",
    "            outputs = self.forward(inputs)\n",
    "            ps = torch.exp(outputs)\n",
    "            p,top = ps.topk(topk, dim=1)\n",
    "        return p,top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The topk method of a tensor in Pytorch returns k indices and their values from a tensor along a dimension (dim=1 means along each row i.e. horizontally. Since our Tensor is 50 x number of classes, this would return the topk classes and their probabilities in each row)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Method\n",
    "This is the main method that is called by the user of our class to kick off training. It implements the main training loop that implements the epoch loop. \n",
    "\n",
    "It calls train_ method, calls validation periodically to monitor performance and overfitting etc., keeps track of best accuracy achieved so far, saves the best accuracy model, saves full model alongwith its hyper-parameters and other variables to disk as a checkpoint.\n",
    "Checkpoints can be restored and training continued later if power is lost or training is disrupted due to some reason.\n",
    "\n",
    "Let's build this method step by step below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Network(nn.Module):\n",
    "    ...\n",
    "    \n",
    "    def fit(self,trainloader,validloader,epochs=2,print_every=10,validate_every=1):\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            '''\n",
    "            We move the model to device ('gpu' or 'cpu')\n",
    "            '''\n",
    "            self.model.to(self.device)\n",
    "            \n",
    "            print('epoch {:3d}/{}'.format(epoch+1,epochs))\n",
    "            \n",
    "            epoch_train_loss =  self.train_(trainloader,self.criterion,\n",
    "                                            self.optimizer,print_every)\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            We check if we need to call validate after every validate_every epochs,\n",
    "            call it and print out the validation loss and accuracy.\n",
    "            '''\n",
    "            if  validate_every and (epoch % validate_every == 0):\n",
    "                t2 = time.time()\n",
    "                epoch_validation_loss,epoch_accuracy = self.validate_(validloader)\n",
    "                time_elapsed = time.time() - t2\n",
    "                print(f\"{time.asctime()}--Validation time {time_elapsed:.3f} seconds..\"\n",
    "                      f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                      f\"Epoch Training loss: {epoch_train_loss:.3f}.. \"\n",
    "                      f\"Epoch validation loss: {epoch_validation_loss:.3f}.. \"\n",
    "                      f\"validation accuracy: {epoch_accuracy:.3f}\")\n",
    "                    \n",
    "                self.train()\n",
    "                \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the best accuracy model\n",
    "The fit function should also monitor the best accuracy achieved so far across all epochs and save the best accuracy model as soon as it gets a new one better than the previous best. This ensure that even without checkpoints, we should be able to retrieve our best model if our validation loss starts to go down while training. \n",
    "\n",
    "This is a common scenario as training may take hours to complete and we may have to leave the system un-attended. This way we could ensure that we always re-load the best accuracy model's weights and use them for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self,device=None):\n",
    "        ...\n",
    "        \n",
    "        '''\n",
    "        we initialize the best_accuracy to 0. when we create the model instance\n",
    "        '''\n",
    "        self.best_accuracy = 0.\n",
    "\n",
    "    ...\n",
    "    \n",
    "    def fit(self,trainloader,validloader,epochs=2,print_every=10,validate_every=1):\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            self.model.to(self.device)\n",
    "            print('epoch {:3d}/{}'.format(epoch+1,epochs))\n",
    "            epoch_train_loss =  self.train_(trainloader,self.criterion,\n",
    "                                            self.optimizer,print_every)\n",
    "                    \n",
    "            if  validate_every and (epoch % validate_every == 0):\n",
    "                t2 = time.time()\n",
    "                epoch_validation_loss,epoch_accuracy = self.validate_(validloader)\n",
    "                time_elapsed = time.time() - t2\n",
    "                print(f\"{time.asctime()}--Validation time {time_elapsed:.3f} seconds..\"\n",
    "                      f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                      f\"Epoch Training loss: {epoch_train_loss:.3f}.. \"\n",
    "                      f\"Epoch validation loss: {epoch_validation_loss:.3f}.. \"\n",
    "                      f\"validation accuracy: {epoch_accuracy:.3f}\")\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                We check and save the new best accuracy in the model if validate \n",
    "                returned a better accuracy.\n",
    "                '''\n",
    "                if self.best_accuracy == 0. or (epoch_accuracy > self.best_accuracy):\n",
    "                    print('updating best accuracy: previous best = {:.3f} new best = {:.3f}'.format(self.best_accuracy,\n",
    "                                                                                     epoch_accuracy))\n",
    "                    self.best_accuracy = epoch_accuracy\n",
    "                    \n",
    "                    '''\n",
    "                    Pytorch save method saves any Pytorch Tensor Data structure by serializing it with \n",
    "                    Python's Pickle Module. Here we are storing the model's state dictionary returned \n",
    "                    by state_dict() method that contains all the weights of the model's full graph \n",
    "                    (each tensor in the architecture).\n",
    "                    '''\n",
    "                    torch.save(self.state_dict(),self.best_accuracy_file)\n",
    "                    \n",
    "                self.train() # just in case we forgot to put the model back to train mode in validate\n",
    "                \n",
    "        print('loading best accuracy model')\n",
    "        \n",
    "        '''\n",
    "        We restore the best accuracy model when we are done with the training loop. \n",
    "        This ensures that any evaluation or inference we perform while the model \n",
    "        remains in memory shall be done using the best accuracy model instead of \n",
    "        the one obtained in the last iteration of the training loop.\n",
    "        '''\n",
    "        self.load_state_dict(torch.load(self.best_accuracy_file))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that the self.best_accuracy_file shall be the filename set during initialization of the model parameters (please see next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting and getting different parameters and hyper-parameters\n",
    "We need to set different parameters and hyper-parameters of a model. These include loss function (criterion), optimizer, dropout probability, learning rate and some others.\n",
    "We write four methods:\n",
    "* set_criterion to create an instance of the loss function and set it on the model\n",
    "* set_optimizer to create an instance of the optimizer and set it on the model\n",
    "* set_model_params that calls the above two functions and sets additional hyper-parameters on the model object\n",
    "* get_model_params that retrieves the currently set parameters on a model. This will come handy when we want to save a full model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Network(nn.Module):\n",
    "    ...\n",
    "    \n",
    "    def set_criterion(self,criterion_name):\n",
    "            if criterion_name.lower() == 'nllloss':\n",
    "                self.criterion_name = 'NLLLoss'\n",
    "                self.criterion = nn.NLLLoss()\n",
    "            elif criterion_name.lower() == 'crossentropyloss':\n",
    "                self.criterion_name = 'CrossEntropyLoss'\n",
    "                self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def set_optimizer(self,params,optimizer_name='adam',lr=0.003):\n",
    "        from torch import optim\n",
    "\n",
    "        if optimizer_name.lower() == 'adam':\n",
    "            print('setting optim Adam')\n",
    "            self.optimizer = optim.Adam(params,lr=lr)\n",
    "            self.optimizer_name = optimizer_name\n",
    "        elif optimizer.lower() == 'sgd':\n",
    "            print('setting optim SGD')\n",
    "            self.optimizer = optim.SGD(params,lr=lr)\n",
    "\n",
    "        elif optimizer.lower() == 'adadelta':\n",
    "            print('setting optim Ada Delta')\n",
    "            self.optimizer = optim.Adadelta(params)\n",
    "            \n",
    "    def set_model_params(self,\n",
    "                         criterion_name,\n",
    "                         optimizer_name,\n",
    "                         lr, # learning rate\n",
    "                         dropout_p,\n",
    "                         model_name,\n",
    "                         best_accuracy,\n",
    "                         best_accuracy_file,\n",
    "                         class_names):\n",
    "        \n",
    "        self.set_criterion(criterion_name)\n",
    "        self.set_optimizer(self.parameters(),optimizer_name,lr=lr)\n",
    "        self.lr = lr\n",
    "        self.dropout_p = dropout_p\n",
    "        self.model_name =  model_name\n",
    "        self.best_accuracy = best_accuracy\n",
    "        self.best_accuracy_file = best_accuracy_file\n",
    "        self.class_names = class_names\n",
    "    \n",
    "    def get_model_params(self):\n",
    "        params = {}\n",
    "        params['device'] = self.device\n",
    "        params['model_name'] = self.model_name\n",
    "        params['optimizer_name'] = self.optimizer_name\n",
    "        params['criterion_name'] = self.criterion_name\n",
    "        params['lr'] = self.lr\n",
    "        params['dropout_p'] = self.dropout_p\n",
    "        params['best_accuracy'] = self.best_accuracy\n",
    "        params['best_accuracy_file'] = self.best_accuracy_file\n",
    "        params['class_names'] = self.class_names\n",
    "        return params\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* set_criterion supports two loss functions: CrossEntropy and NLLLoss. However, support for other loss functions can be trivially added by adding more if else statements.\n",
    "* It is passed the name of the loss function and it instantiates an object using Pytorch API.\n",
    "* set_optimizer similarly enables the optimizer by instantiating it using Pytorch API. It supports 'Adam' as default while SGD and Adadelta can be set. Again support for other optimizers can be easily added.\n",
    "* set_model_params is a higher level method that calls set_criterion and set_optimizer as well as other parameters like model_name, current value of best accuracy, best_accuracy_file where we store the best accuracy, model weights, learning-rate and dropout probability. \n",
    "* We have omitted sanity checking for correctness of the types of parameters for brevity (e.g. model_name, optimizer_name should be strings and dropout_p, lr should be a float etc.).\n",
    "* The set_model_param method shall be called from the main model classes e.g. Transfer Learning and Fully Connected models whose classes we shall next derive from this base Network class\n",
    "* get_model_param simply returns the current parameters as a dicitonary. It will be used in creating the checkpoint (see next).\n",
    "* class_names is a dictionary that contains a mapping of class identifiers (integers) to class names (strings) if such a mapping is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model Checkpoint\n",
    "* Saving a checkpoint of a model is an important task when training Deep Learning models. \n",
    "* This way we can comfortably execute long-running training loops\n",
    "* If there is any disruption e.g. the machine crashes, power fails, Jupyter Notebpook crashes or any other unforeseen issue happens and our training is interrupted, we can restore from the last checkpoint and continue training. Our (potentially) hours of training shall not be lost.\n",
    "\n",
    "* Now we will implement a method, save_checkpoint. \n",
    "* Later in this tutorial we will implement a utility function load_checkpoint when we have the derived classes from this base class for Fully Connected and Transfer Learning Models and we kow which type of model we need to instantiate (we will add that information to the store_chkpoint at that time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    ...\n",
    "    \n",
    "    '''\n",
    "    We add the chkpoint_file argument to the set_params function\n",
    "    '''\n",
    "    def set_model_params(self,\n",
    "                         criterion_name,\n",
    "                         optimizer_name,\n",
    "                         lr, # learning rate\n",
    "                         dropout_p,\n",
    "                         model_name,\n",
    "                         best_accuracy,\n",
    "                         best_accuracy_file,\n",
    "                         chkpoint_file):\n",
    "        \n",
    "        self.criterion_name = criterion_name\n",
    "        self.set_criterion(criterion_name)\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.set_optimizer(self.parameters(),optimizer_name,lr=lr)\n",
    "        self.lr = lr\n",
    "        self.dropout_p = dropout_p\n",
    "        self.model_name =  model_name\n",
    "        self.best_accuracy = best_accuracy\n",
    "        print('set_model_params: best accuracy = {:.3f}'.format(self.best_accuracy))  \n",
    "        self.best_accuracy_file = best_accuracy_file\n",
    "        self.chkpoint_file = chkpoint_file\n",
    "    \n",
    "    def get_model_params(self):\n",
    "        params = {}\n",
    "        params['device'] = self.device\n",
    "        params['model_name'] = self.model_name\n",
    "        params['optimizer_name'] = self.optimizer_name\n",
    "        params['criterion_name'] = self.criterion_name\n",
    "        params['lr'] = self.lr\n",
    "        params['dropout_p'] = self.dropout_p\n",
    "        params['best_accuracy'] = self.best_accuracy\n",
    "        print('get_model_params: best accuracy = {:.3f}'.format(self.best_accuracy))  \n",
    "        params['best_accuracy_file'] = self.best_accuracy_file\n",
    "        params['chkpoint_file'] = self.chkpoint_file\n",
    "        print('get_model_params: chkpoint file = {}'.format(self.chkpoint_file))  \n",
    "        return params\n",
    "    \n",
    "    def save_chkpoint(self):\n",
    "        saved_model = {}\n",
    "        '''\n",
    "        We retrieve all params via get_model_params and also the class names and \n",
    "        just dump them in the chkpoint file.\n",
    "        '''\n",
    "        saved_model['params'] = self.get_model_params()    \n",
    "        torch.save(saved_model,self.chkpoint_file)\n",
    "        print('checkpoint created successfully in {}'.format(self.chkpoint_file))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create a Fully Connected Class derived from the Base Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create our first derived class for Fully Connected Neural Networks. Fully Connected Networks are traditionally called Multi-layer Perceptrons (MLP) in the literature. In most Deep Learning frameworks (including Pytorch) they are simply called Linear layers.\n",
    "\n",
    "* In order to have a functional class for a Fully Connected Network, we will rely on Pytorch's nn.Linear Module. \n",
    "\n",
    "* nn.Linear module is itself derived from nn.Module from which we derived our own Network class.\n",
    "* A Fully Connected Network consists of three basic pieces:\n",
    "  * inputs \n",
    "  * Fully Connected hidden layers with each one followed by a non-linear transformation (let's consider the non-linearity as part of the hidden layer instead of treating it as a separate layer)\n",
    "  \n",
    "  * An output layer and the number of outputs\n",
    "  \n",
    "### Fully Connected Network Requirements\n",
    "We need to meet the following requirements to create such a class:\n",
    " *  Ability to specify as many hidden layers as desired\n",
    " *  Ability to specify the number of inputs and outputs of the model\n",
    " *  Ability to define drop out and non-linearity ('relu', tanh etc.) for each layer\n",
    " *  Ability to define the output layer and prepare it for the classification task\n",
    " *  Set different parameters and hyper-parameters of the model like optimizer, loss function etc.\n",
    " \n",
    " Given these requirements, let's define a class for Fully Connected Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Our new class FC is derived (inherited) from our base class Network and not nn.Module. \n",
    "So it gets all the methods available in base class for free\n",
    "'''\n",
    "class FC(Network):\n",
    "    '''\n",
    "    We pass the model parameters to init. We have already seen most of the parameters. \n",
    "    The additional ones are the num_inputs, num_outputs, non_linearity which is \n",
    "    defaulted to 'relu' and a list of hidden layer dimensions.\n",
    "    '''\n",
    "    def __init__(self,num_inputs,\n",
    "                 num_outputs,\n",
    "                 layers=[],\n",
    "                 lr=0.003,\n",
    "                 class_names=None,\n",
    "                 optimizer_name='Adam',\n",
    "                 dropout_p=0.2,\n",
    "                 non_linearity='relu',\n",
    "                 criterion_name='NLLLoss',\n",
    "                 model_type='classifier',\n",
    "                 best_accuracy=0.,\n",
    "                 best_accuracy_file ='best_accuracy.pth',\n",
    "                 chkpoint_file ='chkpoint_file.pth',\n",
    "                 device=None):\n",
    "        \n",
    "        super().__init__(device=device)\n",
    "        \n",
    "        self.set_model_params(criterion_name,\n",
    "                              optimizer_name,\n",
    "                              lr,\n",
    "                              dropout_p,\n",
    "                              'FC',\n",
    "                              best_accuracy,\n",
    "                              best_accuracy_file,\n",
    "                              chkpoint_file\n",
    "                              )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **num_inputs** is the total number of input features this Network is going to accept.\n",
    "* **num_outputs** is the total number of outputs this Network is going to emit after passing throuhg any hidden layers. In other words this is the dimension of the output layer.\n",
    "* **Non-linearity** is stored in the model as an attribute. Note that we do not pass the non-linearity to set_model_params as this is model specific and does not belong to the base class. \n",
    "* We may have to implement our versions of set_model_params and get_model_params methods later if we want to set and get additional parameters specific to the model. This is like implementing our own **\"__ init __\"** and then calling the parent's too. We do additional work in our code and then call the parent to do the common work.\n",
    "* **layers** is a list specifying the number of units in each hidden layer. The order of numbers in this list would also specify their order in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Network using nn.Sequential\n",
    "* **nn.Sequential** is a Pytorch method to create a simple sequential Neural Network that just concatenates the defined modules together as a sequence.\n",
    "* At the time of execution, nn.Sequential automatically calls the forward methods of each module in the sequence.\n",
    "\n",
    "Here we define an empty nn.Sequential first and then add the input module, hidden layers and output module to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(Network):\n",
    "    def __init__(self,num_inputs,\n",
    "                 num_outputs,\n",
    "                 layers=[],\n",
    "                 lr=0.003,\n",
    "                 class_names=None,\n",
    "                 optimizer_name='Adam',\n",
    "                 dropout_p=0.2,\n",
    "                 non_linearity='relu',\n",
    "                 criterion_name='NLLLoss',\n",
    "                 model_type='classifier',\n",
    "                 best_accuracy=0.,\n",
    "                 best_accuracy_file ='best_accuracy.pth',\n",
    "                 chkpoint_file ='chkpoint_file.pth',\n",
    "                 device=None):\n",
    "                              \n",
    "        super().__init__(device=device)\n",
    "        \n",
    "        self.set_model_params(criterion_name,\n",
    "                              optimizer_name,\n",
    "                              lr,\n",
    "                              dropout_p,\n",
    "                              'FC',\n",
    "                              best_accuracy,\n",
    "                              best_accuracy_file,\n",
    "                              chkpoint_file\n",
    "                              )\n",
    "        \n",
    "        self.non_linearity = non_linearity\n",
    "        \n",
    "        '''\n",
    "        We store the actual Network as a Sequential block in model attribute of our FC object\n",
    "        '''\n",
    "        self.model = nn.Sequential()\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        We create groups of layers and add them to the Sequential model. \n",
    "        Each group consists of a linear layer followed by a non-linearity\n",
    "        and dropout with probability passed as an argument.                                                         inplace=True))\n",
    "\n",
    "        '''\n",
    "        if len(layers) > 0:\n",
    "            \n",
    "            self.model.add_module('fc1',nn.Linear(num_inputs,layers[0]))\n",
    "            self.model.add_module('relu1',nn.ReLU())\n",
    "            self.model.add_module('dropout1',nn.Dropout(p=dropout_p,inplace=True))\n",
    "\n",
    "            for i in range(1,len(layers)):\n",
    "                self.model.add_module('fc'+str(i+1),nn.Linear(layers[i-1],layers[i]))\n",
    "                self.model.add_module('relu'+str(i+1),nn.ReLU())\n",
    "                self.model.add_module('dropout'+str(i+1),nn.Dropout(p=dropout_p,\n",
    "           \n",
    "            self.model.add_module('out',nn.Linear(layers[-1],num_outputs))\n",
    "                                                                    \n",
    "        else:\n",
    "                                                                    \n",
    "        '''\n",
    "        If we don't have any hidden layer we just add one layer to our sequential model with number of \n",
    "        inputs and number of outputs. In this case we don't add any non-linearity or dropout since \n",
    "        non-Linearity is typically added in hidden layers.\n",
    "        '''                                                            \n",
    "            self.model.add_module('out',nn.Linear(num_inputs,num_outputs))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **nn.Linear** is a Pytorch class that takes the number of inputs and the number of outputs and creates a Linear model with internal forward function.\n",
    "* Note that we are naming our output layer as **'out'** and our hidden layers as **'fcX'** where X is the layer number (1, 2 ..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions for Classification\n",
    "* We can broadly divide Linear Networks into two type: Regression and Classification.\n",
    "* Although there are many loss functions used for classification, two most common ones which can be generlized easily from 2 to any number of classes are:\n",
    "  * Negative Likehood Log Loss or NLLLoss\n",
    "  * CrossEntropy Loss\n",
    "  \n",
    "### NLLLoss\n",
    "* The NLLLoss function is very simple. It assumes that its input is a probability. It just takes the -ve of the log of its input for each input and adds them up. (more about it here: https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/ )\n",
    "* We need to convert the outputs to probabilities before feeding to the NLLLoss.\n",
    "* The simplest way to do that is to take the Softmax of the inputs by taking the exponent of each input and dividing by the sum of the exponents (more info on the same link above). After this operation the outputs can be interpreted as probabilities (because they have been scaled or calibrated between 0 and 1), which are then fed to the NLLLoss, which outputs sum(-log(p)) where p is the output of each probability.\n",
    "* However, in Pytorch the NLLLoss function expects that the log has already been calculated and it just puts a -ve sign and sums up the inputs. Therefore, we need to take the log ourselves after Softmax. There is a convenient function in Pytorch called LogSoftmax that does exactly that. So we will use it if our loss function is specified to be 'NLLLoss' by adding that after our output layer in the Sequential.\n",
    "\n",
    "### Cross_entropy Loss\n",
    "* If we were using Cross-Entropy loss we do nothing as the CrossEntropyLoss function will do what's required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(Network):\n",
    "    def __init__(self,num_inputs,\n",
    "                 num_outputs,\n",
    "                 layers=[],\n",
    "                 lr=0.003,\n",
    "                 class_names=None,\n",
    "                 optimizer_name='Adam',\n",
    "                 dropout_p=0.2,\n",
    "                 non_linearity='relu',\n",
    "                 criterion_name='NLLLoss',\n",
    "                 model_type='classifier',\n",
    "                 best_accuracy=0.,\n",
    "                 best_accuracy_file ='best_accuracy.pth',\n",
    "                 chkpoint_file ='chkpoint_file.pth',\n",
    "                 device=None):\n",
    "        \n",
    "        super().__init__(device=device)\n",
    "        \n",
    "        self.set_model_params(criterion_name,\n",
    "                              optimizer_name,\n",
    "                              lr,\n",
    "                              dropout_p,\n",
    "                              'FC',\n",
    "                              best_accuracy,\n",
    "                              best_accuracy_file,\n",
    "                              chkpoint_file\n",
    "                              )\n",
    "        \n",
    "        self.non_linearity = non_linearity\n",
    "        \n",
    "        self.model = nn.Sequential()\n",
    "        \n",
    "        if len(layers) > 0:\n",
    "            self.model.add_module('fc1',nn.Linear(num_inputs,layers[0]))\n",
    "            self.model.add_module('relu1',nn.ReLU())\n",
    "            self.model.add_module('dropout1',nn.Dropout(p=dropout_p,inplace=True))\n",
    "\n",
    "            for i in range(1,len(layers)):\n",
    "                self.model.add_module('fc'+str(i+1),nn.Linear(layers[i-1],layers[i]))\n",
    "                self.model.add_module('relu'+str(i+1),nn.ReLU())\n",
    "                self.model.add_module('dropout'+str(i+1),nn.Dropout(p=dropout_p,\n",
    "                                                                    inplace=True))\n",
    "\n",
    "            self.model.add_module('out',nn.Linear(layers[-1],num_outputs))\n",
    "        else:\n",
    "            self.model.add_module('out',nn.Linear(num_inputs,num_outputs))\n",
    "        \n",
    "        '''\n",
    "        We use Logsoftmax if loss = NLLLoss\n",
    "        '''\n",
    "        if model_type.lower() == 'classifier' and criterion_name.lower() == 'nllloss':\n",
    "            self.model.add_module('logsoftmax',nn.LogSoftmax(dim=1))\n",
    "            \n",
    "        '''\n",
    "         We save the attributes of the model in our object for possible later reference.\n",
    "        '''\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.layer_dims = layers\n",
    "        \n",
    "        '''\n",
    "        We store the class names dictionary if passed, otherwise we create a simple dictionary\n",
    "        with each class id converted to an string id e.g. 1 shall be converted to \n",
    "        '1' as class name.\n",
    "        '''\n",
    "        if class_names is not None:\n",
    "            self.class_names = class_names\n",
    "        else:\n",
    "            self.class_names = {str(k):v for k,v in enumerate(list(range(num_outputs)))}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening the inputs\n",
    " * Before we could feed inputs to our FC Network, we need to flatten the input Tensor so that each row is just a one dimensional tensor and we have a batch of those rows. In other words the inputs have to be two dimensional (rows by columns) as most of you might be familiar with tabular data (from CSV files for example), used in Machine Learning. This is a requirement of the Linear Layer that it expects its data to be in batches of single dimensional tensors (vectors).\n",
    "\n",
    "* To achieve this we simply have to change the view of our input tensors (if they are already in two dimensional nothing will change in the view).\n",
    "\n",
    "* To do so we define a simple one-liner function as a utility. This makes the code much more readable as we immediately know that a flattening operation is going on instead of a rather cryptic .view statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_tensor(x):\n",
    "    return x.view(x.shape[0],-1)\n",
    "\n",
    "class FC(Network):\n",
    "    def __init__(self,num_inputs,\n",
    "                 num_outputs,\n",
    "                 layers=[],\n",
    "                 lr=0.003,\n",
    "                 class_names=None,\n",
    "                 optimizer_name='Adam',\n",
    "                 dropout_p=0.2,\n",
    "                 non_linearity='relu',\n",
    "                 criterion_name='NLLLoss',\n",
    "                 model_type='classifier',\n",
    "                 best_accuracy=0.,\n",
    "                 best_accuracy_file ='best_accuracy.pth',\n",
    "                 chkpoint_file ='chkpoint_file.pth',\n",
    "                 device=None):\n",
    "        \n",
    "        \n",
    "        super().__init__(device=device)\n",
    "        \n",
    "        self.set_model_params(criterion_name,\n",
    "                              optimizer_name,\n",
    "                              lr,\n",
    "                              dropout_p,\n",
    "                              'FC',\n",
    "                              best_accuracy,\n",
    "                              best_accuracy_file,\n",
    "                              chkpoint_file\n",
    "                              )\n",
    "        \n",
    "        \n",
    "        self.non_linearity = non_linearity\n",
    "        \n",
    "        self.model = nn.Sequential()\n",
    "        \n",
    "        if len(layers) > 0:\n",
    "            self.model.add_module('fc1',nn.Linear(num_inputs,layers[0]))\n",
    "            self.model.add_module('relu1',nn.ReLU())\n",
    "            self.model.add_module('dropout1',nn.Dropout(p=dropout_p,inplace=True))\n",
    "\n",
    "            for i in range(1,len(layers)):\n",
    "                self.model.add_module('fc'+str(i+1),nn.Linear(layers[i-1],layers[i]))\n",
    "                self.model.add_module('relu'+str(i+1),nn.ReLU())\n",
    "                self.model.add_module('dropout'+str(i+1),nn.Dropout(p=dropout_p,\n",
    "                                                                    inplace=True))\n",
    "\n",
    "            self.model.add_module('out',nn.Linear(layers[-1],num_outputs))\n",
    "        else:\n",
    "            self.model.add_module('out',nn.Linear(num_inputs,num_outputs))\n",
    "        \n",
    "        if model_type.lower() == 'classifier' and criterion_name.lower() == 'nllloss':\n",
    "            self.model.add_module('logsoftmax',nn.LogSoftmax(dim=1))\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.layer_dims = layers\n",
    "        \n",
    "        if class_names is not None:\n",
    "            self.class_names = class_names\n",
    "        else:\n",
    "            self.class_names = {str(k):v for k,v in enumerate(list(range(num_outputs)))}\n",
    "    \n",
    "    '''\n",
    "    We define our forward function that basically calls the forward function of our model\n",
    "    (nn.Sequential in this case) after flattening the inputs.\n",
    "    '''\n",
    "    def forward(self,x):\n",
    "        return self.model(flatten_tensor(x))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting and Getting Dropout\n",
    "* We add two more convenience methods that give us the ability to change dropout probability any time we want.\n",
    "* This might come in handy when we want to experiment quickly with different dropout probability values or may be change dropout dynamically while training based on some condition e.g. on detecting heavy overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FC(Network):\n",
    "    ...\n",
    "    '''\n",
    "    Dropout layers in Pytorch are of type 'torch.nn.modules.dropout.Dropout'. \n",
    "    This can be checked for and acted upon accordingly for each such layer \n",
    "    in our sequential model.\n",
    "    '''\n",
    "    def _get_dropout(self):\n",
    "        for layer in self.model:\n",
    "            if type(layer) == torch.nn.modules.dropout.Dropout:\n",
    "                return layer.p\n",
    "            \n",
    "    def _set_dropout(self,p=0.2):\n",
    "        for layer in self.model:\n",
    "            if type(layer) == torch.nn.modules.dropout.Dropout:\n",
    "                print('FC: setting dropout prob to {:.3f}'.format(p))\n",
    "                layer.p=p\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we are checking each layer for this type of module and if true we act accordingly in set and get methods.\n",
    "* Note that the **torch.nn.modules.dropout.Dropout** has an attribute p where the dropout probability is stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are four additional attributes of our FC model we need to save in order to restore it correctly.\n",
    "These are numb_inputs,num_outputs, layers and class_names. Since these are quite specific to FC model, we shuld write FC model's versions of get_model_param and set_model_param methods that internally call the base class ones and also perform any additional stuff.\n",
    "\n",
    "So let's do that and complete our class before writing our restore_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(Network):\n",
    "    ...\n",
    "    \n",
    "    def set_model_params(self,\n",
    "                         criterion_name,\n",
    "                         optimizer_name,\n",
    "                         lr,\n",
    "                         dropout_p,\n",
    "                         model_name,\n",
    "                         model_type,\n",
    "                         best_accuracy,\n",
    "                         best_accuracy_file,\n",
    "                         chkpoint_file,\n",
    "                         num_inputs,\n",
    "                         num_outputs,\n",
    "                         layers,\n",
    "                         class_names):\n",
    "        \n",
    "        '''\n",
    "        We call the parent class's set_model_params method passing it all its\n",
    "        required arguments, and then add additional parameters to our \n",
    "        object as attributes.\n",
    "        '''\n",
    "        \n",
    "        super(FC, self).set_model_params(criterion_name,\n",
    "                              optimizer_name,\n",
    "                              lr,\n",
    "                              dropout_p,\n",
    "                              model_name,\n",
    "                              best_accuracy,\n",
    "                              best_accuracy_file,\n",
    "                              chkpoint_file\n",
    "                              )\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.layer_dims = layers\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        if class_names is not None:\n",
    "            self.class_names = class_names\n",
    "        else:\n",
    "            self.class_names = {k:str(v) for k,v in enumerate(list(range(num_outputs)))}\n",
    "        \n",
    "    def get_model_params(self):\n",
    "        '''\n",
    "        We call the parent class's get_model_params method and retrieve the dictionary \n",
    "        of params, then add our model specific attributes to the dictionary\n",
    "        '''\n",
    "        params = super(FC, self).get_model_params()\n",
    "        params['num_inputs'] = self.num_inputs\n",
    "        params['num_outputs'] = self.num_outputs\n",
    "        params['layers'] = self.layer_dims\n",
    "        params['model_type'] = self.model_type\n",
    "        params['class_names'] = self.class_names\n",
    "        params['device'] = self.device\n",
    "        return params\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a Saved Checkpoint\n",
    "* Now let's create a load_chkpoint utility function which is given a checkpoint file to retrieve the model parameters and reconstruct the appropriate model. Since we have only one model type right now **(FC)**, we will put a check for that model_type only and later add support for Transfer Learning and any other classes as we create them.\n",
    "* The code is pretty straight forward. It gets the params dictionary from the chkpoint_file and calls the appropriate constructor and finally loads the state dictionary of the best accuracy model from the filename of the best accuracy model retrieved from the chkpoint_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chkpoint(chkpoint_file):\n",
    "        \n",
    "    restored_data = torch.load(chkpoint_file)\n",
    "\n",
    "    params = restored_data['params']\n",
    "    print('load_chkpoint: best accuracy = {:.3f}'.format(params['best_accuracy']))  \n",
    "    \n",
    "    if params['model_type'].lower() == 'classifier':\n",
    "        net = FC( num_inputs=params['num_inputs'],\n",
    "                  num_outputs=params['num_outputs'],\n",
    "                  layers=params['layers'],\n",
    "                  device=params['device'],\n",
    "                  criterion_name = params['criterion_name'],\n",
    "                  optimizer_name = params['optimizer_name'],\n",
    "                  model_name = params['model_name'],\n",
    "                  lr = params['lr'],\n",
    "                  dropout_p = params['dropout_p'],\n",
    "                  best_accuracy = params['best_accuracy'],\n",
    "                  best_accuracy_file = params['best_accuracy_file'],\n",
    "                  chkpoint_file = params['chkpoint_file'],\n",
    "                  class_names =  params['class_names']\n",
    "          )\n",
    "\n",
    "    net.load_state_dict(torch.load(params['best_accuracy_file']))\n",
    "\n",
    "    net.to(params['device'])\n",
    "    \n",
    "    return net\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This completes our FC class**. Now we should test it before proceeding further. Let's test it on **MNIST** dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we should calculate the MNIST dataset's **mean and std** values. They can be calculated without getting into any memory issues in a couple seconds with the function we created earlier for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0839, 0.2038, 0.1042]), tensor([0.2537, 0.3659, 0.2798]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = datasets.MNIST(root='data',download=False,\n",
    "                            transform = transforms.transforms.ToTensor())\n",
    "mean_,std_= calculate_img_stats(train_data)\n",
    "mean_,std_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We create the transforms as before using the calculated mean and std values, and then apply them to our train and test sets, and then split our train set into train and validation. Remember that our split_image_data function just converts the test set into a dataloader if it is given as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomRotation(10),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.0839, 0.2038, 0.1042],[0.2537, 0.3659, 0.2798])\n",
    "                                     ])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.0839, 0.2038, 0.1042],[0.2537, 0.3659, 0.2798])\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='data',download=False,train=True, transform = train_transform)\n",
    "test_dataset = datasets.MNIST(root='data',download=False,train=False,transform = test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 240, 200)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader,validloader,testloader = split_image_data(train_dataset,test_dataset,batch_size=50)\n",
    "len(trainloader),len(validloader),len(testloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We create an FC layer with number of inputs = 784 which is obtained after flattening the image dimensions (1 x 28 x 28) and number of outputs = 10 since we have 10 classes (digits 0 to 9)\n",
    "* We arbitrarily select two hidden layer of 512 units each\n",
    "* We set the optimizer to Ada Delta (more on it next)\n",
    "* We set best accuracy and checkpoint files as appropriate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting optim Ada Delta\n"
     ]
    }
   ],
   "source": [
    "net =  FC(num_inputs=784,\n",
    "          num_outputs=10,\n",
    "          layers=[512,512],\n",
    "          optimizer_name='Adadelta',\n",
    "          best_accuracy_file ='best_accuracy_mnist_fc_test.pth',\n",
    "          chkpoint_file ='chkpoint_file_mnist_fc_test.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer choices\n",
    "* Optimizer algorithms come in many variations and forms. Most of them try to optimize the basic gradient descent algorithm by varying the learning rate and other related parameters as they see the data.\n",
    "* A full survey of optimizers is beyond the scope of this tutorial. See here for a detailed overview (http://ruder.io/optimizing-gradient-descent). \n",
    "* The main difference between most frequently used ones (adapted from the link above) are as follows:\n",
    "  * **Batch Gragient Descend** is simplest and performs weight updates after looking at the entire dataset\n",
    "  * **SGD** (Stochastic Gradient Descent) is on the other extreme and performs weight updates for each item (training example) in the dataset\n",
    "  * **Mini-batch GD** is a variant of SGD and takes the best of both worlds. It updates weights after each mini-batch of data. In other words, pure SGD is mini-batch with a batch-size of 1. Anything in between 1 and the entire dataset, we call it Mini-batch GD\n",
    "  * **Momentum** is a method that helps accelerate SGD in the relevant direction and attempts to dampen too many oscillations when trying to converge to a minimum\n",
    "  * **Adagrad** adapts the learning rate to the parameters, and applies different learning rates for updating different parameters, based on the past history of the squares of the magnitude of gradients of each prameter. The main advantage of Adagrad is that the user does not have to tune the learning rate manually\n",
    "  * **Adadelta** is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size \"w\"\n",
    "  * **RMSprop** is an unpublished, adaptive learning rate method proposed by **Geoff Hinton** in Lecture 6e of his **Coursera** Class (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf). RMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad's radically diminishing learning rates\n",
    "  * **Adaptive Moment Estimation (Adam)** is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients, similar to momentum\n",
    "\n",
    "In my experimentation, **Adadelta** gives the highest accuracy on image datasets in general, much better than Adam and SGD, especially on Cifar10, although I admit I haven't tried Adadelta and RMSProp much. you should try these with your experiments to see if they make any difference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we call our fit function passing it train and validation dataloaders and train for 5 epochs, printing every 300 batches each epoch while performing validaton every epoch (remember that default of validate_every = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(trainloader,validloader,epochs=5,print_every=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the following output (partial output shown for brevity)\n",
    "\n",
    "**updating best accuracy: previous best = 95.883 new best = 95.992**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we got reasonable accuracy **(95.99)** in only **5** epochs. May be we can squeeze some more juice out of it by training for a few more epochs. \n",
    "\n",
    "So let' first test our save and load chkpoint functions and then continue training for another 10 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_model_params: best accuracy = 95.992\n",
      "get_model_params: chkpoint file = chkpoint_file_mnist_fc_test.pth\n",
      "checkpoint created successfully in chkpoint_file_mnist_fc_test.pth\n"
     ]
    }
   ],
   "source": [
    "net.save_chkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the saved chkpoint into another variable to ensure that it is a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_chkpoint: best accuracy = 95.992\n",
      "setting optim Ada Delta\n"
     ]
    }
   ],
   "source": [
    "net2 = load_chkpoint('chkpoint_file_mnist_fc_test.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2.fit(trainloader,validloader,epochs=10,print_every=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**updating best accuracy: previous best = 96.392 new best = 96.875**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best accuracy we could achieve on validation set after another 10 epochs is 96.875. Let's save and restore the model one more time before testing our evaluate method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_model_params: best accuracy = 96.875\n",
      "get_model_params: chkpoint file = chkpoint_file_mnist_fc_test.pth\n",
      "checkpoint created successfully in chkpoint_file_mnist_fc_test.pth\n"
     ]
    }
   ],
   "source": [
    "net2.save_chkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_chkpoint: best accuracy = 96.875\n",
      "setting optim Ada Delta\n"
     ]
    }
   ],
   "source": [
    "net3 = load_chkpoint('chkpoint_file_mnist_fc_test.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96.95,\n",
       " [('0', 98.87755102040816),\n",
       "  ('1', 99.03083700440529),\n",
       "  ('2', 96.70542635658916),\n",
       "  ('3', 94.75247524752474),\n",
       "  ('4', 97.35234215885947),\n",
       "  ('5', 95.73991031390135),\n",
       "  ('6', 96.4509394572025),\n",
       "  ('7', 96.78988326848248),\n",
       "  ('8', 97.1252566735113),\n",
       "  ('9', 96.33300297324084)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net3.evaluate(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also test our predict function next. To do that, we need to convert our testloader into a Python iterator, and then get the next batch from it using \"next\" method of the iterator. If you are not familiar with Python iterators, please see any good tutorial such as here (https://www.datacamp.com/community/tutorials/python-iterator-tutorial) for more information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(testloader)\n",
    "imgs_,labels_ = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), 7)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_[0].shape,labels_[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that first image of our first batch is 1x28x28 while its label = 7\n",
    "We can verify this by displaying the image using matplotlib's pyplot library after converting the image to numpy and removing the extra dimension to make it only 28 x 28 instead of 1 x 28 x 28\n",
    "\n",
    "Note that to convert a Pytorch Tensor to numpy array, simply use .numpy() method available on Pytorch Tensor objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f62408487b8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "fig = plt.figure(figsize=(40,10))\n",
    "ax = fig.add_subplot(2,10, 1, xticks=[], yticks=[])\n",
    "ax.imshow(np.squeeze(imgs_[0].numpy()), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our model predicts the image correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net3.predict(imgs_[0])[1].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ok so our evaluate and predict methods seem to be working fine, and we are able to score around **97%** on test set with all individual accuracies in the mid 90s. \n",
    "* This is pretty good given that we have only trained for **15** epochs for less than **3 minutes** and are using a simple fully connected network without any fancy **CNN** stuff\n",
    "* Additionally, we have refactored our code into classes, utility functions and are also able to save and restore models as we require."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Create a Transfer Learning Class derived from the Base Class\n",
    "Now we are ready to create our Transfer Learning class derived from our base **Network** class. Implementing **Transfer Learning** will turn out to be incredibly simple now with all the required machinery already in place. \n",
    "\n",
    "Transfer Learning class is based on torchvision.models module that contains support for  downloading and using several pretrained Network architectures for Computer Vision. We are going to add support for three models: \n",
    "  * Densenet121 which we simply call DenseNet\n",
    "  * Resent34 and ResNet50 respectively\n",
    "\n",
    "We have the option to use the pretrained versions of models (passing pretrained = True which is default anyway) where we obtain the architecture + weights, or just the architectures without weights and train them from scratch.\n",
    "\n",
    "* Most of the pretrained versions available in torchvision.models have been trained on ImageNet with 1000 output classes\n",
    "* We want to adapt the selected model to our use case. For example, for CIFAR10, we only need 10 classes, so our output should be set to 10 instead of 1000\n",
    "\n",
    "* Each model can be considered as composed of two parts:\n",
    "    * Convolution Backbone (a CNN architecture with several blocks comprising of convolutions with varying number of filters, non-linearities, max or average pooling layers, batch normalizations, dropout layers etc.)\n",
    "    * A head with fully connected classifier at the output end\n",
    "    \n",
    "* In most cases, the output layer does not have any fully connected hidden layers \n",
    "* However, we have the option to replace the classifier layer with our own and add more hidden layers by replacing the output layer with our own. We may easily use our own FC class (defined earlier in this tutorial) for this purpose \n",
    "* On the other hand, we may choose to just change the number of outputs without adding any additional hidden layers\n",
    "* Henceforth, we are going to use our own FC class and replace the original model's output layer with an FC object. This would give us the flexibility to pass any additional hidden layer if we want.\n",
    "\n",
    "The code for our new class 'TransferNetworkImg' (derived from out base class 'Network') is quite simple. You just have to pay attention to two functions:\n",
    "   * set_transfer_model which sets the Transfer model from torchvision.models\n",
    "   * set_model_head which sets the FC layer on the model after removing the original classifier or fc layer.\n",
    "\n",
    "### Setting the Classifier\n",
    "* Note that the classifier at the head of each model is named differently in each torchvision model. Although, there are better ways to handle it, such as using a predefined dictionary in a file and loading it and looking up the classifier field for each model type to get the output layer's name\n",
    "\n",
    "* However, in the following code, we are just using simple if else statements. You are welcome to create your own versions of this calss by creating such a dictionary if you want\n",
    "\n",
    "* Setting the FC model is done in set_model_head. Since we need to call the FC constructor, we need to pass anything that is required to create our FC class object successfully. We are doing that by passing a dictionary called 'head' to our Transfer Learning Class\n",
    "\n",
    "* To successfully create an FC model, we need to pass a fixed number of inputs to its constructor since it is a requirement of our nn.Linear layer used in FC Networks. Luckily, the nn.Linear class in Pytorch stores its number of inputs in an attribute called 'in_features'. We can grab that from the original classifier layer in the transferred model (Densenet, Resnet etc.) and pass it as argument to our FC constructor\n",
    "\n",
    "\n",
    "### Freezing and un-freezing layers\n",
    "* When using Transfer Learning models, it is important to decide whether we want to retrain all the layers (including Convolutions and Fully Connected) from scratch on our dataset. For reasonably large datasets such as CIFAR10, it makes sense to retrain the whole network.\n",
    "* However, please note that retraining all layers does not mean we are going to start from random weights. We will still start with pretrained weights of each layer and continue from there, but we will be calculating the gradients for all layers and updating all weights. So in other words, the model starts learning while keeping the knowledge it gained for identifying images when it trained on the previous dataset (ImageNet in most cases).\n",
    "* So its like a child that we trained on a specific thing and we don't want it to throw away all knowledge and continue when looking at new data.\n",
    "* On the other hand, we may want to keep the weights frozen for the backbone while retraining on the head only. This is a common scenario when we have trained the network on new data for a while and now our backbone knows both ImageNet and our new dataset (CIFAR10 in our case).\n",
    "* In the last case, we may want to do only predictions and want all weights including the backbone and the head to remain frozen. This is only good for prediction and evaluation though and not for training since there is no point in training if we don't want to do back propagation and update nothing.\n",
    "\n",
    "We write a function to freeze weights while keeping the head unfrozen by default using the Pytorch Tensor's requires_grad flag. This flag is available in all Tensors and we want to set it True or False for weight Tensors (which could be obtained via parameters() method of any model derived from nn.Module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding support for Freeze and Unfreeze in our base class\n",
    "We need to add support for Freeze and Unfreeze in our base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "   ...\n",
    "'''\n",
    "We have added two methods to freeze and unfreeze the parameters of all layers of our model\n",
    "'''\n",
    "    def freeze(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        \n",
    "    def unfreeze(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferNetworkImg(Network):\n",
    "    def __init__(self,\n",
    "                 model_name='DenseNet',\n",
    "                 lr=0.003,\n",
    "                 criterion_name ='NLLLoss',\n",
    "                 optimizer_name = 'Adam',\n",
    "                 dropout_p=0.2,\n",
    "                 pretrained=True,\n",
    "                 device=None,\n",
    "                 best_accuracy=0.,\n",
    "                 best_accuracy_file ='best_accuracy.pth',\n",
    "                 chkpoint_file ='chkpoint_file',\n",
    "                 head={}):\n",
    "\n",
    "        \n",
    "        super().__init__(device=device)\n",
    "        \n",
    "        self.model_type = 'transfer'\n",
    "        \n",
    "        self.set_transfer_model(model_name,pretrained=pretrained)    \n",
    "        \n",
    "        if head is not None:\n",
    "            self.set_model_head(model_name = model_name,\n",
    "                                 head = head,\n",
    "                                 optimizer_name = optimizer_name,\n",
    "                                 criterion_name = criterion_name,\n",
    "                                 lr = lr,\n",
    "                                 dropout_p = dropout_p,\n",
    "                                 device = device\n",
    "                                )\n",
    "            \n",
    "        self.set_model_params(criterion_name,\n",
    "                              optimizer_name,\n",
    "                              lr,\n",
    "                              dropout_p,\n",
    "                              model_name,\n",
    "                              best_accuracy,\n",
    "                              best_accuracy_file,\n",
    "                              chkpoint_file,\n",
    "                              head)\n",
    "            \n",
    "    \n",
    "    '''\n",
    "    set_model_params calls the parent's method as before and sets \n",
    "    additional attributes specific to this class \n",
    "    (head and model_type set to 'transfer')\n",
    "    '''\n",
    "    def set_model_params(self,criterion_name,\n",
    "                         optimizer_name,\n",
    "                         lr,\n",
    "                         dropout_p,\n",
    "                         model_name,\n",
    "                         best_accuracy,\n",
    "                         best_accuracy_file,\n",
    "                         chkpoint_file,\n",
    "                         head):\n",
    "        \n",
    "        print('Transfer: best accuracy = {:.3f}'.format(best_accuracy))\n",
    "        \n",
    "        super(TransferNetworkImg, self).set_model_params(\n",
    "                                              criterion_name,\n",
    "                                              optimizer_name,\n",
    "                                              lr,\n",
    "                                              dropout_p,\n",
    "                                              model_name,\n",
    "                                              best_accuracy,\n",
    "                                              best_accuracy_file,\n",
    "                                              chkpoint_file\n",
    "                                              )\n",
    "\n",
    "        '''\n",
    "        We also set the head end of our model to create FC layer\n",
    "        when required\n",
    "        '''\n",
    "        self.head = head\n",
    "        \n",
    "        '''\n",
    "        this time our model_type is transfer and not classifier\n",
    "        '''\n",
    "        self.model_type = 'transfer'\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "        \n",
    "    def get_model_params(self):\n",
    "        params = super(TransferNetworkImg, self).get_model_params()\n",
    "        params['head'] = self.head\n",
    "        params['model_type'] = self.model_type\n",
    "        params['device'] = self.device\n",
    "        return params\n",
    "    \n",
    "    '''\n",
    "    Freeze first freezes all paramters of our model by calling the base class's freeze() \n",
    "    method which we have added to the Network class (see below) and then unfreezes the \n",
    "    head's (classifier attribute) parameters based on the passed flag. \n",
    "    Note that we are calling our head as classifier. We would have to add more code if \n",
    "    we want to handle the case of regression as well in future. We have added two methods,\n",
    "    freeze and unfreeze to our base class appropriately\n",
    "    '''\n",
    "    def freeze(self,train_classifier=True):\n",
    "        super(TransferNetworkImg, self).freeze()\n",
    "        if train_classifier:\n",
    "            for param in self.model.classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "                \n",
    "    def set_transfer_model(self,mname,pretrained=True):   \n",
    "        self.model = None\n",
    "        if mname.lower() == 'densenet':\n",
    "            self.model = models.densenet121(pretrained=pretrained)\n",
    "            \n",
    "        elif mname.lower() == 'resnet34':\n",
    "            self.model = models.resnet34(pretrained=pretrained)\n",
    "            \n",
    "        elif mname.lower() == 'resnet50':\n",
    "            self.model = models.resnet50(pretrained=pretrained)\n",
    "              \n",
    "        if self.model is not None:\n",
    "            print('set_transfer_model: self.Model set to {}'.format(mname))\n",
    "        else:\n",
    "            print('set_transfer_model:Model {} not supported'.format(mname))\n",
    "            \n",
    "    \n",
    "    '''\n",
    "    set_model_head calls the FC constructors using the head dictionary. \n",
    "    It grabs the in_features from the appropriate attribute of the original\n",
    "    model's classifier or fc layer. \n",
    "    \n",
    "    We need to check if the model was saved and loaded from a checkpoint \n",
    "    because in the later case, the model's head-end object shall have \n",
    "    num_inputs attribute instead of the original in_features because \n",
    "    it would contain our FC object and that has num_inputs in \n",
    "    place of in_features.\n",
    "\n",
    "    '''\n",
    "    def set_model_head(self,\n",
    "                        model_name = 'DenseNet',\n",
    "                        head = {'num_inputs':128,\n",
    "                                'num_outputs':10,\n",
    "                                'layers':[],\n",
    "                                'class_names':{}\n",
    "                               },\n",
    "                         optimizer_name = 'Adam',\n",
    "                         criterion_name = 'NLLLoss',\n",
    "                         lr = 0.003,\n",
    "                         dropout_p = 0.2,\n",
    "                         device = None):\n",
    "        \n",
    "        self.num_outputs = head['num_outputs']\n",
    "        \n",
    "        if model_name.lower() == 'densenet':\n",
    "            if hasattr(self.model,'classifier'):\n",
    "                in_features =  self.model.classifier.in_features\n",
    "            else:\n",
    "                in_features = self.model.classifier.num_inputs\n",
    "                \n",
    "            self.model.classifier = FC(num_inputs=in_features,\n",
    "                                       num_outputs=head['num_outputs'],\n",
    "                                       layers = head['layers'],\n",
    "                                       class_names = head['class_names'],\n",
    "                                       non_linearity = head['non_linearity'],\n",
    "                                       model_type = head['model_type'],\n",
    "                                       model_name = head['model_name'],\n",
    "                                       dropout_p = dropout_p,\n",
    "                                       optimizer_name = optimizer_name,\n",
    "                                       lr = lr,\n",
    "                                       criterion_name = criterion_name,\n",
    "                                       device=device\n",
    "                                      )\n",
    "            \n",
    "        elif model_name.lower() == 'resnet50' or model_name.lower() == 'resnet34':\n",
    "            if hasattr(self.model,'fc'):\n",
    "                in_features =  self.model.fc.in_features\n",
    "            else:\n",
    "                in_features = self.model.fc.num_inputs\n",
    "                \n",
    "            self.model.fc = FC(num_inputs=in_features,\n",
    "                               num_outputs=head['num_outputs'],\n",
    "                               layers = head['layers'],\n",
    "                               class_names = head['class_names'],\n",
    "                               non_linearity = head['non_linearity'],\n",
    "                               model_type = head['model_type'],\n",
    "                               model_name = head['model_name'],\n",
    "                               dropout_p = dropout_p,\n",
    "                               optimizer_name = optimizer_name,\n",
    "                               lr = lr,\n",
    "                               criterion_name = self.criterion_name,\n",
    "                               device=device\n",
    "                              )\n",
    "         \n",
    "        self.head = head\n",
    "        \n",
    "        print('{}: setting head: inputs: {} hidden:{} outputs: {}'.format(model_name,\n",
    "                                                                   in_features,\n",
    "                                                                   head['layers'],\n",
    "                                                                   head['num_outputs']))\n",
    "    \n",
    "    def _get_dropout(self):\n",
    "        if self.model_name.lower() == 'densenet':\n",
    "            return self.model.classifier._get_dropout()\n",
    "        \n",
    "        elif self.model_name.lower() == 'resnet50' or self.model_name.lower() == 'resnet34':\n",
    "            return self.model.fc._get_dropout()\n",
    "        \n",
    "            \n",
    "    def _set_dropout(self,p=0.2):\n",
    "        \n",
    "        if self.model_name.lower() == 'densenet':\n",
    "            if self.model.classifier is not None:\n",
    "                print('DenseNet: setting head (FC) dropout prob to {:.3f}'.format(p))\n",
    "                self.model.classifier._set_dropout(p=p)\n",
    "                \n",
    "        elif self.model_name.lower() == 'resnet50' or self.model_name.lower() == 'resnet34':\n",
    "            if self.model.fc is not None:\n",
    "                print('ResNet: setting head (FC) dropout prob to {:.3f}'.format(p))\n",
    "                self.model.fc._set_dropout(p=p)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding support for Transfer Learning Model to load_chkpoint utility\n",
    "* We need to add the case for our TransferNetworkImg case in load_chkpoint function.\n",
    "* The main addition is the storage and retrieval of head along with other params and also adding support for passing the retrieved head to the constructor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chkpoint(chkpoint_file):\n",
    "        \n",
    "    restored_data = torch.load(chkpoint_file)\n",
    "\n",
    "    params = restored_data['params']\n",
    "    print('load_chkpoint: best accuracy = {:.3f}'.format(params['best_accuracy']))  \n",
    "    \n",
    "    if params['model_type'].lower() == 'classifier':\n",
    "        net = FC( num_inputs=params['num_inputs'],\n",
    "                  num_outputs=params['num_outputs'],\n",
    "                  layers=params['layers'],\n",
    "                  device=params['device'],\n",
    "                  criterion_name = params['criterion_name'],\n",
    "                  optimizer_name = params['optimizer_name'],\n",
    "                  model_name = params['model_name'],\n",
    "                  lr = params['lr'],\n",
    "                  dropout_p = params['dropout_p'],\n",
    "                  best_accuracy = params['best_accuracy'],\n",
    "                  best_accuracy_file = params['best_accuracy_file'],\n",
    "                  chkpoint_file = params['chkpoint_file'],\n",
    "                  class_names =  params['class_names']\n",
    "          )\n",
    "    elif params['model_type'].lower() == 'transfer':\n",
    "        net = TransferNetworkImg(criterion_name = params['criterion_name'],\n",
    "                                 optimizer_name = params['optimizer_name'],\n",
    "                                 model_name = params['model_name'],\n",
    "                                 lr = params['lr'],\n",
    "                                 device=params['device'],\n",
    "                                 dropout_p = params['dropout_p'],\n",
    "                                 best_accuracy = params['best_accuracy'],\n",
    "                                 best_accuracy_file = params['best_accuracy_file'],\n",
    "                                 chkpoint_file = params['chkpoint_file'],\n",
    "                                 head = params['head']\n",
    "                               )\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    net.load_state_dict(torch.load(params['best_accuracy_file']))\n",
    "\n",
    "    net.to(params['device'])\n",
    "    \n",
    "    return net\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Train two different pretrained, transferred models on Cifar10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move on to testing and experimentation. But before we can do that we should move our code to .py files and import it as modules. This makes it much more convenient and we don't have to rerun al lthe notebook cells after every time we reset the Python kernel of our notebook to empty the GPU memory for a fresh run.\n",
    "\n",
    "I have created four files:\n",
    "1. **model.py (contains the core Network class)**\n",
    "2. **fc.py (contains the FC class)**\n",
    "3. **cv_model.py (contains the TransferNetworkImg class)**\n",
    "4. **utils.py (contains all the utility functions not belonging to any class)**\n",
    "\n",
    "We create these files in a folder called mylib and import all of them. \n",
    "\n",
    "We also should use a special directive of our Jupyter notebook that makes it monitor and reload all the imported files in a cell that change on the disk. This would come in handy if wee modify any of the files for any reasn e.g. to fix a bug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mylib.utils import *\n",
    "from mylib.model import *\n",
    "from mylib.cv_model import *\n",
    "from mylib.fc import *\n",
    "from mylib.chkpoint import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Experimentation\n",
    "In the following cells, we are going to perform the following steps in a sequence:\n",
    "\n",
    "* Create our classes dictionary as well as the head dictionary to pass to the Transfer Learning object's constructor\n",
    "* Create a Transfer Learning object for Densenet\n",
    "* Unfreeze it\n",
    "* Fit it to train for 3 epochs\n",
    "* Save the check-point\n",
    "* Load it back into another variable\n",
    "* Unfreeze again and repeat with 3 more epochs\n",
    "* Save the check-point again\n",
    "* Reload into another variable\n",
    "* Freeze this time and retrain for 3 more epochs\n",
    "* Save the model again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "class_dict = {k:v for k,v in enumerate(classes)}\n",
    "\n",
    "head={\n",
    "       'num_outputs':10,\n",
    "       'layers':[],\n",
    "        'class_names':class_dict,\n",
    "         'non_linearity':'relu',\n",
    "         'model_type':'classifier',\n",
    "         'model_name':'FC'\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_densenet = TransferNetworkImg(model_name='DenseNet',\n",
    "                   optimizer_name = 'Adadelta',               \n",
    "                   best_accuracy_file ='densenet_best_accuracy_cifar10.pth',\n",
    "                   chkpoint_file ='densenet_cifar10_chkpoint_file',\n",
    "                   head = head\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_densenet.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_densenet.fit(trainloader,validloader,epochs=3,print_every=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**updating best accuracy: previous best = 80.490 new best = 85.920**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_model_params: best accuracy = 85.920\n",
      "get_model_params: chkpoint file = densenet_cifar10_chkpoint_file\n",
      "checkpoint created successfully in densenet_cifar10_chkpoint_file\n"
     ]
    }
   ],
   "source": [
    "transfer_densenet.save_chkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_chkpoint: best accuracy = 85.920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/.conda/envs/dreamai/lib/python3.7/site-packages/torchvision-0.2.1-py3.7.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set_transfer_model: self.Model set to DenseNet\n",
      "setting optim Ada Delta\n",
      "DenseNet: setting head: inputs: 1024 hidden:[] outputs: 10\n",
      "Transfer: best accuracy = 85.920\n",
      "setting optim Ada Delta\n"
     ]
    }
   ],
   "source": [
    "transfer_densenet2 = load_chkpoint('densenet_cifar10_chkpoint_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_densenet2.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_densenet2.fit(trainloader,validloader,epochs=3,print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_model_params: best accuracy = 90.770\n",
      "get_model_params: chkpoint file = densenet_cifar10_chkpoint_file\n",
      "checkpoint created successfully in densenet_cifar10_chkpoint_file\n"
     ]
    }
   ],
   "source": [
    "transfer_densenet2.save_chkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This time we have crossed 90% accuracy after unfreezing and training for another 3 epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_chkpoint: best accuracy = 90.770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/.conda/envs/dreamai/lib/python3.7/site-packages/torchvision-0.2.1-py3.7.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set_transfer_model: self.Model set to DenseNet\n",
      "setting optim Ada Delta\n",
      "DenseNet: setting head: inputs: 1024 hidden:[] outputs: 10\n",
      "Transfer: best accuracy = 90.770\n",
      "setting optim Ada Delta\n"
     ]
    }
   ],
   "source": [
    "transfer_densenet3 = load_chkpoint('densenet_cifar10_chkpoint_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_densenet3.freeze()\n",
    "transfer_densenet3.fit(trainloader,validloader,epochs=3,print_every=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**updating best accuracy: previous best = 94.940 new best = 95.080**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_model_params: best accuracy = 95.080\n",
      "get_model_params: chkpoint file = densenet_cifar10_chkpoint_file\n",
      "checkpoint created successfully in densenet_cifar10_chkpoint_file\n"
     ]
    }
   ],
   "source": [
    "transfer_densenet3.save_chkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So after 9 epochs, 6 with unfreeze and 3 with freeze, we are at 95.08%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's repeat the same steps with Resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set_transfer_model: self.Model set to ResNet34\n",
      "setting optim Ada Delta\n",
      "ResNet34: setting head: inputs: 512 hidden:[] outputs: 10\n",
      "Transfer: best accuracy = 0.000\n",
      "setting optim Ada Delta\n"
     ]
    }
   ],
   "source": [
    "transfer_resnet = TransferNetworkImg(model_name='ResNet34',\n",
    "                   optimizer_name = 'Adadelta',               \n",
    "                   best_accuracy_file ='resnet34_best_accuracy_cifar10.pth',\n",
    "                   chkpoint_file ='resnet34_cifar10_chkpoint_file',\n",
    "                   head = head\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_resnet.unfreeze()\n",
    "transfer_resnet.fit(trainloader,validloader,epochs=3,print_every=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**updating best accuracy: previous best = 82.700 new best = 86.040**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_model_params: best accuracy = 86.040\n",
      "get_model_params: chkpoint file = resnet34_cifar10_chkpoint_file\n",
      "checkpoint created successfully in resnet34_cifar10_chkpoint_file\n"
     ]
    }
   ],
   "source": [
    "transfer_resnet.save_chkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_chkpoint: best accuracy = 86.040\n",
      "set_transfer_model: self.Model set to ResNet34\n",
      "setting optim Ada Delta\n",
      "ResNet34: setting head: inputs: 512 hidden:[] outputs: 10\n",
      "Transfer: best accuracy = 86.040\n",
      "setting optim Ada Delta\n"
     ]
    }
   ],
   "source": [
    "transfer_resnet2 = load_chkpoint('resnet34_cifar10_chkpoint_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_resnet2.unfreeze()\n",
    "transfer_resnet2.fit(trainloader,validloader,epochs=3,print_every=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**updating best accuracy: previous best = 89.400 new best = 89.640**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_model_params: best accuracy = 89.640\n",
      "get_model_params: chkpoint file = resnet34_cifar10_chkpoint_file\n",
      "checkpoint created successfully in resnet34_cifar10_chkpoint_file\n"
     ]
    }
   ],
   "source": [
    "transfer_resnet2.save_chkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_chkpoint: best accuracy = 89.640\n",
      "set_transfer_model: self.Model set to ResNet34\n",
      "setting optim Ada Delta\n",
      "ResNet34: setting head: inputs: 512 hidden:[] outputs: 10\n",
      "Transfer: best accuracy = 89.640\n",
      "setting optim Ada Delta\n"
     ]
    }
   ],
   "source": [
    "transfer_resnet3 = load_chkpoint('resnet34_cifar10_chkpoint_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transfer_resnet3.freeze()\n",
    "transfer_resnet3.fit(trainloader,validloader,epochs=3,print_every=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**updating best accuracy: previous best = 94.280 new best = 94.580**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_model_params: best accuracy = 94.580\n",
      "get_model_params: chkpoint file = resnet34_cifar10_chkpoint_file\n",
      "checkpoint created successfully in resnet34_cifar10_chkpoint_file\n"
     ]
    }
   ],
   "source": [
    "transfer_resnet3.save_chkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Evaluate and predict on test set with individual models and Ensemble\n",
    "We load both files for final Densenet and Resnet models and evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_densenet = load_chkpoint('densenet_cifar10_chkpoint_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transfer: best accuracy = 95.08**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93.0,\n",
       " [('airplane', 94.69999999999999),\n",
       "  ('automobile', 96.5),\n",
       "  ('bird', 90.4),\n",
       "  ('cat', 84.5),\n",
       "  ('deer', 94.3),\n",
       "  ('dog', 89.5),\n",
       "  ('frog', 94.5),\n",
       "  ('horse', 94.69999999999999),\n",
       "  ('ship', 94.5),\n",
       "  ('truck', 96.39999999999999)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_densenet.evaluate(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_resnet = load_chkpoint('resnet34_cifar10_chkpoint_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transfer: best accuracy = 94.580**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92.52,\n",
       " [('airplane', 94.5),\n",
       "  ('automobile', 96.8),\n",
       "  ('bird', 89.3),\n",
       "  ('cat', 82.89999999999999),\n",
       "  ('deer', 94.0),\n",
       "  ('dog', 87.2),\n",
       "  ('frog', 95.6),\n",
       "  ('horse', 95.0),\n",
       "  ('ship', 94.19999999999999),\n",
       "  ('truck', 95.7)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_resnet.evaluate(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling multiple models to improve accuracy\n",
    "* We have tested and evaluated two different \"transferred\" models. Both models seem to perform almost equally well on this dataset. We might wonder what would happen if we somehow combine the results of both models to make our final prediciton. Combining two or more models together is called Ensemble learning. \n",
    "\n",
    "* You might have heard the term in case of traditional ML with Random Forests (RF) and Gradient Boosted Decision Trees (GBDT) as Ensemble models. Here we are talking about using two or more Deep Learning models to try to achieve better accuracy. For more information, please see [Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)\n",
    "\n",
    "* The intuition behind Ensembling is that one model might have mis-classified  a specific example while predicting but one or more of the others might have got it right. Our final prediction accuracy would likely improve if we somehow combine the predictions.\n",
    "\n",
    "* One simple way to combine the predicitions is to give weights to each model's predictions based on some heuristic such as:\n",
    "    1. Simple averaging of predicted values (e.g. probabilities) of different ensembles\n",
    "    2. Assigning different weights to each member of an Ensemble based on its performance on the validation set\n",
    "    3. Assign weights based on our experience with the model in general on multiple datasets. If one model performs better in majority of cases, we should give its prediction more weight.\n",
    "\n",
    "* A generalized way to create an Ensemble could be to create an Emseble model class derived from our base Network class just like for Transfer-Learning and FC. \n",
    "\n",
    "* We don't need to have fit and train methods since the members of our Ensemble are expected to be pretrained outside the Ensemble itself. However, implementing the predict and evaluate methods in the Ensemble class makes sense. \n",
    "\n",
    "* We could pass the model objects to the Ensemble along with their weights while constructing it. In a highly desirable scenario, we would like an ML model of some sort to learn those weights themselves, but to keep things simpler (at least in this tutorial), we would pass weights by using some heuristic as discussed. \n",
    "\n",
    "* We could then write evaluate and predict methods such that they call each individual member's corresponding methods and multiply them by the model's given weight, and add the wieghted predictions to make the final one.\n",
    "\n",
    "Below we show the relevant code of such a class. The code is quite simple and salient parts are explained for further clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(Network):\n",
    "    '''\n",
    "    The constructor expects a list of models and each member of list to be a tuple.\n",
    "    First element of tuple must be the pretrained model object and the second the\n",
    "    weight of the model.\n",
    "    '''\n",
    "    def __init__(self,models):\n",
    "        self.criterion = None\n",
    "        super().__init__()\n",
    "        self.models = models\n",
    "        '''\n",
    "        The weights must sum to 1 so that our predictions are a weighted sum of\n",
    "        the predictions of all models for each class.\n",
    "        '''\n",
    "        if sum(model[1] for model in models) != 1.0:\n",
    "            raise ValueError('Weights of Ensemble must sum to 1')\n",
    "            \n",
    "        \n",
    "    def evaluate(self,testloader,metric='accuracy'):\n",
    "        from collections import defaultdict\n",
    "        #evaluations = defaultdict(float)\n",
    "        #num_classes = self.models[0][0].num_outputs\n",
    "        class_correct = defaultdict(int)\n",
    "        class_totals = defaultdict(int)\n",
    "\n",
    "        class_names = self.models[0][0].class_names  \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for inputs, labels in testloader:\n",
    "                ps_list = []  \n",
    "                '''\n",
    "                 We go in a loop calling the forward method\n",
    "                 of each model and multiply the predicted \n",
    "                 values by the model's weight.\n",
    "                '''\n",
    "                for model in self.models:\n",
    "                    model[0].eval()\n",
    "                    model[0].to(model[0].device)\n",
    "                    inputs, labels = inputs.to(model[0].device), labels.to(model[0].device)\n",
    "                    outputs = model[0].forward(inputs)\n",
    "                    ps = torch.exp(outputs)\n",
    "                    ps = ps * model[1] # multiply by model's weight\n",
    "                    \n",
    "                    '''\n",
    "                    We build a list of the predicted probabilities and then go through\n",
    "                    the list performing the sum of those values. \n",
    "                    Since the predictions had been already multiplied by weights in the\n",
    "                    previous loop, we just need to sum now to get the final weighted sum. \n",
    "                    \n",
    "                    The vector of weighted sum contains the ensembled predicted values\n",
    "                    for each class. We can get the max of it just like we do for our \n",
    "                    regular models to get the final prediction for this image.\n",
    "                    '''\n",
    "                    ps_list.append(ps)\n",
    "                    \n",
    "                final_ps = ps_list[0]\n",
    "                for i in range(1,len(ps_list)):\n",
    "                    final_ps = final_ps + ps_list[i]\n",
    "                _, final_preds = torch.max(final_ps, 1)\n",
    "                #print(final_preds)\n",
    "                update_classwise_accuracies(final_preds,labels,class_correct,class_totals)\n",
    "        \n",
    "       \n",
    "        \n",
    "        return get_accuracies(class_names,class_correct,class_totals)\n",
    "                   \n",
    "    '''\n",
    "    Predict is very similar to the regular predict. Again the only difference\n",
    "    is that we have the two loops and the final prediction is the \n",
    "    topk of the ensembled weighted sum of predictions.\n",
    "    '''\n",
    "    def predict(self,inputs,topk=1):\n",
    "        ps_list = []  \n",
    "        for model in self.models:\n",
    "            model[0].eval()\n",
    "            model[0].to(model[0].device)\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(model[0].device)\n",
    "                outputs = model[0].forward(inputs)\n",
    "                ps_list.append(torch.exp(outputs)*model[1])\n",
    "       \n",
    "        final_ps = ps_list[0]\n",
    "        for i in range(1,len(ps_list)):\n",
    "            final_ps = final_ps + ps_list[i]\n",
    "        \n",
    "        _,top = final_ps.topk(topk, dim=1)\n",
    "            \n",
    "        return top\n",
    "    \n",
    "    def forward(self,x):\n",
    "        outputs = []\n",
    "        for model in self.models:\n",
    "             outputs.append(model[0].forward(x))\n",
    "        return outputs\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating with Ensemble Models\n",
    "* Let's create an Ensemble object and give 0.5 weight to both of ur models since they don't differ by much and observe the improvement in performance (if any)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94.22,\n",
       " [('airplane', 95.6),\n",
       "  ('automobile', 97.8),\n",
       "  ('bird', 92.0),\n",
       "  ('cat', 86.6),\n",
       "  ('deer', 96.0),\n",
       "  ('dog', 89.8),\n",
       "  ('frog', 96.2),\n",
       "  ('horse', 96.5),\n",
       "  ('ship', 95.3),\n",
       "  ('truck', 96.39999999999999)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble = EnsembleModel([(transfer_densenet,0.5),(transfer_resnet,0.5)])\n",
    "ensemble.evaluate(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So we go get a significant improvement by using the Ensemble.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8:  Predict on Kaggle's given much larger Test set\n",
    "\n",
    "The original CIFAR10 dataset has 60000 images, 50000 in the train set and 10000 in the test set. However, Kaggle has provided a huge dataset of 300000 images for testing of CIFAR10. Here is what Kaggle web-site has to say about these images:\n",
    "\n",
    "\"To discourage certain forms of cheating (such as hand labeling) we have added 290,000 junk images in the test set. These images are ignored in the scoring. We have also made trivial modifications to the official 10,000 test images to prevent looking them up by file hash. These modifications should not appreciably affect the scoring. You should predict labels for all 300,000 images.\"\n",
    "\n",
    "Unzipping this test dataset once it has downloaded takes an enormous amount of time (several hours on my machine). \n",
    "\n",
    "## Creating our own custom Dataset for Kaggle test images\n",
    "In order to handle this dataset, we have written our own custom dataset class derived from the base Dataset class of Pytorch. We then pass this dataset object to the Pytorch Dataloader. This makes it much convenient in handling this large dataset. It also gives us good practice in creating our own Dataset for images.\n",
    "\n",
    "Below is the code for our own custom dataset class. The code is pretty straight forward. \n",
    "* A typical customer dataset contains an \"__ init __\" method, a \"__ getitem__\" method to convert it into an iterator and an \"__ len __ \" method to make the Python's len() function work on the dataset.\n",
    "\n",
    "* Our custom dataset class assumes that information about the dataset image files is contained in a CSV file.\n",
    "* The image ids are contained in column 0 of the file while image filename (path) is in column 1 and image's label (if available in the file) as a text e.g. bird, plane etc. is contained in column 2. \n",
    "* Our Kaggle test set has no labels since Kaggle uses it for scoring the competition and therefore, does not provide labels for test sets.\n",
    "* We use Pandas Dataframe to handle the CSV file and then create the actual image set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_path,\n",
    "                 transforms=None,\n",
    "                 labels_=False):\n",
    "       \n",
    "        self.labels = None\n",
    "        self.transforms = None\n",
    "\n",
    "        '''\n",
    "        We read the csv file in a Pandas DataFrame and extract image ids, image file-paths \n",
    "        and labels (if present) from its columns, assuming that they are contained in columns\n",
    "        0, 1 and 2 respectively.\n",
    "        '''\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        \n",
    "        self.ids = np.asarray(self.df.iloc[:, 0])\n",
    "        \n",
    "        self.images = np.asarray(self.df.iloc[:, 1])\n",
    "        \n",
    "        if labels:\n",
    "            self.labels = np.asarray(self.df.iloc[:, 1])\n",
    "        \n",
    "        '''\n",
    "        We set the length of the dataset as well as a transform if passed\n",
    "        '''\n",
    "        self.data_len = len(self.df.index)\n",
    "        if transforms is not None:\n",
    "            self.transforms = transforms\n",
    "            \n",
    "        #print(self.data_len)\n",
    "\n",
    "    '''\n",
    "    In the __ getitem__ we have to read a single image from its file according to the index requested in. \n",
    "    Remember that this function shall be called by the Dataloader when creating a batch. It would call it\n",
    "    for each ieration of batch consturction loop. We return the image and position index on each such call.\n",
    "    '''\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image_name = self.images[index]\n",
    "        id_ = self.ids[index]\n",
    "        img_ = Image.open(image_name)\n",
    "        \n",
    "        '''\n",
    "        We apply transforms on each channel of the image. [:3,:,:] means all rows and columns\n",
    "        of all three channels if any tranforms were given in the constructor.\n",
    "        '''\n",
    "        if self.transforms is not None:\n",
    "            img_ = self.transforms(img_)[:3,:,:]\n",
    "        \n",
    "        '''\n",
    "        Just to keep the API consistent and always return a two-valued tuple, we return 0 as label with\n",
    "        each image even if there was no label. The assumption is that the caller of this method knows \n",
    "        if label is to be expected. \n",
    "        '''\n",
    "        label = 0\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[index]\n",
    "        \n",
    "        '''\n",
    "        Our Dataset object is returning a three member tuple (id,image,label)\n",
    "        '''\n",
    "        return (id_,img_,label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Meta-Data of the Test Set into a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to use our Custom Dataset class with a Pytorch Dataloader, we need to create the csv file we want to pass to the Dataset constructor. \n",
    "\n",
    "* Kaggle hasn't really given usa csv file. All we have is a folder of images. To create the csv file we need to parse the image filenames and store the image names (ids) in first clumn and the path in the second column of our csv file. \n",
    "\n",
    "* Below is a function that uses  Python's glob module, with Pandas Dataframe and some Python string searching functions to create such a csv file. The code is quite straightforward so we won't describe it here for brevity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_from_folder(folder_path,outfile,cols=['id','path']):\n",
    "    \n",
    "    f = glob.glob(folder_path+'/*.*')\n",
    "    \n",
    "    ids = []\n",
    "    for elem in f:\n",
    "        t = elem[elem.rfind('/')+1:]\n",
    "        ids.append(t[:t.rfind('.')])\n",
    "    data = {cols[0]:ids,cols[1]:f}    \n",
    "    df = pd.DataFrame(data,columns=cols)\n",
    "    df.to_csv(outfile,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using this function we can create our csv file, passing it the folder and the desired output csv filename as arguments. We have placed our test images from Kaggle in cifar10-test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_from_folder('cifar10-test','cifar10-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can test our code to see s sample of contents from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11798</td>\n",
       "      <td>cifar10-test/11798.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>298292</td>\n",
       "      <td>cifar10-test/298292.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222281</td>\n",
       "      <td>cifar10-test/222281.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165990</td>\n",
       "      <td>cifar10-test/165990.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100937</td>\n",
       "      <td>cifar10-test/100937.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>197039</td>\n",
       "      <td>cifar10-test/197039.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59773</td>\n",
       "      <td>cifar10-test/59773.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52364</td>\n",
       "      <td>cifar10-test/52364.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>240916</td>\n",
       "      <td>cifar10-test/240916.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>244203</td>\n",
       "      <td>cifar10-test/244203.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                     path\n",
       "0   11798   cifar10-test/11798.png\n",
       "1  298292  cifar10-test/298292.png\n",
       "2  222281  cifar10-test/222281.png\n",
       "3  165990  cifar10-test/165990.png\n",
       "4  100937  cifar10-test/100937.png\n",
       "5  197039  cifar10-test/197039.png\n",
       "6   59773   cifar10-test/59773.png\n",
       "7   52364   cifar10-test/52364.png\n",
       "8  240916  cifar10-test/240916.png\n",
       "9  244203  cifar10-test/244203.png"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cifar10-test.csv')\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and preparing the submission file for Kaggle Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we need to do is create our custom Dataset and a Dataloader to perform evaluation on it using our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transform_cifar10 = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(cifar10_mean,cifar10_std)\n",
    "                                    ])\n",
    "\n",
    "cifar10_test_dset = ImageDataset('cifar10-test.csv',transforms=test_transform_cifar10)\n",
    "len(cifar10_test_dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As expected, the Dataset has 300000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11798</td>\n",
       "      <td>cifar10-test/11798.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>298292</td>\n",
       "      <td>cifar10-test/298292.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222281</td>\n",
       "      <td>cifar10-test/222281.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165990</td>\n",
       "      <td>cifar10-test/165990.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100937</td>\n",
       "      <td>cifar10-test/100937.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>197039</td>\n",
       "      <td>cifar10-test/197039.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59773</td>\n",
       "      <td>cifar10-test/59773.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52364</td>\n",
       "      <td>cifar10-test/52364.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>240916</td>\n",
       "      <td>cifar10-test/240916.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>244203</td>\n",
       "      <td>cifar10-test/244203.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                     path\n",
       "0   11798   cifar10-test/11798.png\n",
       "1  298292  cifar10-test/298292.png\n",
       "2  222281  cifar10-test/222281.png\n",
       "3  165990  cifar10-test/165990.png\n",
       "4  100937  cifar10-test/100937.png\n",
       "5  197039  cifar10-test/197039.png\n",
       "6   59773   cifar10-test/59773.png\n",
       "7   52364   cifar10-test/52364.png\n",
       "8  240916  cifar10-test/240916.png\n",
       "9  244203  cifar10-test/244203.png"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10_test_dset.df[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_test_testloader = DataLoader(cifar10_test_dset, batch_size=50,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(cifar10_test_testloader)\n",
    "id_,images_,_ = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 3, 224, 224])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As expected, our Dataloader's one  batch has correct dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit to Kaggle we need to create a csv file with image-id (name) in first column and label in the second (please see the competition web-page here to see the sample submission file).\n",
    "\n",
    "The easiest way to do that is again to use Pandas DataFrame to prepare the results and the file.\n",
    "\n",
    "Below we have the normal Dataloader loop:\n",
    "  * We get the next batch of data. Remember that our Dataset object is returning a three member tuple (id,image,label). We are ignoring the label in this case since we are always returning 0\n",
    "  * We first predict using our Ensemble, convert the predictions Tensor back to CPU, then convert it to numpy, flatten it using numpy's own flatten method available on numpy arrays, and finally convert to a simple Python list. This gives us the predicted classes for the whole batch\n",
    "  * We keep collecting the predictions in our list and the corresponding labels in another list (using a lookup of image_ids into our class dictionary)\n",
    "  * We finally create a Pandas DataFrame with two required columns and write it as a CSV file to disk. To match the exact required format, we set the index to False\n",
    "  * Next we sort the values according to ids as the sample file shows us and rewrite the CSV file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "image_ids = []\n",
    "for ids_,images_,_ in cifar10_test_testloader:\n",
    "    preds_ = ensemble.predict(images_).cpu().numpy().flatten().tolist()\n",
    "    predictions += [class_dict[pred] for pred in preds_]\n",
    "    image_ids += ids_.numpy().flatten().tolist()\n",
    "\n",
    "pd.DataFrame({'id':image_ids,'label':predictions}).to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('submission.csv')\n",
    "df = df.sort_values('id')\n",
    "df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When I submitted to Kaggle, these are the results I got:\n",
    "    * 0.947 with an Ensemble of three models (I added support for another model, not shown in this tutorial)\n",
    "    * 0.945 with an Ensemble of the two models discussed in this tutorial\n",
    "    <img src=\"blog/image.png\"> \n",
    "    * This would have taken me to 3rd place on Kaggle and also pretty high on the benchmarks published on several sites for CIFAR10.\n",
    "    * It took me less than 75 mins to train all the models and create the Ensemble)\n",
    "    * Of course if you play around and spend a bit more time I am sure you can beat the top benchmark\n",
    "* During the course of trying to achieve high accuracy on CIFAR10, we created a resuable set of classes and utilily functions that could be used on any Image Classification task\n",
    "\n",
    "### The key takeaways or a recipe for high accuracy are:\n",
    "   * Use the mean and std of the image set itself instead of ImageNet to normalize if you have a large enough image set\n",
    "   * Use an adaptive learning rate algorithm as optimizer. Adadelta worked best for me on CIFAR10 as well as flowers, and many other so far\n",
    "   * Unfreeze and train for 3 to 5 epochs. Then save, reset the notebook, unfreese again and retrain for 3 to 5 epochs, then freeze and train for another 2 to 3 epochs, and you should be done\n",
    "   * Experiment with additional FC layers and see if it makes any difference\n",
    "   * Train multiple models, save them and use them as an Ensemble to make final predictions\n",
    "   * Perhaps most importantly, consider applied Machine Learning as much of a Software Engineering discipline as a statistical and mathematical one. This means that most of the software engineering best practices still apply. Thinking about ML problems this way would lead you to **design good reusable components like classes and  utility functions to create your own API** even if you are given some code to start with. \n",
    "   * Refactor the code (if given to you) and put it inside your own classes and modify it accordingly to create clean interfaces and abstractions. This will save huge duplication of effort on other datasets and you may also be able to use your classes as components in other larger projects and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
